"""
recipe_search.py â€” ë ˆì‹œí”¼ ê²€ìƒ‰ ë° ì¬ë£Œ ì¶”ì²œ ëª¨ë“ˆ 
ì±…ì„:
- ì‹œë‚˜ë¦¬ì˜¤ 1: ì¼ë°˜ ë ˆì‹œí”¼ ê²€ìƒ‰ (Tavily API) í›„ ì‚¬ì´ë“œë°”ì— ê²°ê³¼ URL í‘œì‹œ
- ì‹œë‚˜ë¦¬ì˜¤ 2: íŠ¹ì • ë ˆì‹œí”¼ ì„ íƒ ì‹œ, URL í¬ë¡¤ë§ ë° LLMì„ í†µí•œ ì¬ë£Œ/ì¡°ë¦¬ë²• ì¶”ì¶œ
- ì¶”ì¶œëœ ì¬ë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‡¼í•‘ëª° ìƒí’ˆ(SKU)ì„ DBì—ì„œ ê²€ìƒ‰í•˜ì—¬ ì‚¬ì´ë“œë°”ì— ì œì•ˆ
- ìµœì¢… ì‘ë‹µ ë©”ì‹œì§€(AIMessage)ë¥¼ í”„ë¡ íŠ¸ì—”ë“œ ê·œê²©ì— ë§ëŠ” 'response' í‚¤ë¡œ í¬ë§·íŒ…
"""
import random
import logging
import os
import requests
import re
import json
from typing import Dict, Any, List, Optional
import mysql.connector
from mysql.connector import Error
from bs4 import BeautifulSoup

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (í™˜ê²½ì— ë§ê²Œ ì¡°ì •)
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from graph_interfaces import ChatState

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("RECIPE_SEARCH")

# --- í™˜ê²½ ë³€ìˆ˜ ë° í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ---
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

try:
    import openai
    openai_api_key = os.getenv("OPENAI_API_KEY")
    openai_client = openai.OpenAI(api_key=openai_api_key) if openai_api_key else None
    if not openai_client:
        logger.warning("OpenAI API key not found. LLM-based features will be disabled.")
except ImportError:
    openai_client = None
    logger.warning("OpenAI package not available. LLM-based features will be disabled.")

# --- DB ì„¤ì • ---
DB_CONFIG = {
    'host': os.getenv('DB_HOST', '127.0.0.1'),
    'user': os.getenv('DB_USER', 'qook_user'),
    'password': os.getenv('DB_PASS', 'qook_pass'),
    'database': os.getenv('DB_NAME', 'qook_chatbot'),
    'port': int(os.getenv('DB_PORT', 3306))
}

# --- ë©”ì¸ ë¼ìš°íŒ… í•¨ìˆ˜ ---
def recipe_search(state: ChatState) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ë‘ ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤ ì¤‘ í•˜ë‚˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    1. ì¼ë°˜ ë ˆì‹œí”¼ ê²€ìƒ‰
    2. ì„ íƒëœ ë ˆì‹œí”¼ì˜ ì¬ë£Œ ì¶”ì²œ
    """
    logger.info("ë ˆì‹œí”¼ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    query = state.query

    try:
        # ì‹œë‚˜ë¦¬ì˜¤ 2: ì‚¬ìš©ìê°€ ì‚¬ì´ë“œë°”ì—ì„œ íŠ¹ì • ë ˆì‹œí”¼ì˜ 'ì¬ë£Œ ì¶”ì²œë°›ê¸°'ë¥¼ í´ë¦­í•œ ê²½ìš°
        if "ì„ íƒëœ ë ˆì‹œí”¼:" in query and "URL:" in query:
            logger.info("ì‹œë‚˜ë¦¬ì˜¤ 2: ì„ íƒëœ ë ˆì‹œí”¼ ì¬ë£Œ ì¶”ì²œ ì‹œì‘")
            recipe = _handle_selected_recipe(query)
            return recipe
        
        # ì‹œë‚˜ë¦¬ì˜¤ 1: ì¼ë°˜ì ì¸ ë ˆì‹œí”¼ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°
        else:
            logger.info("ì‹œë‚˜ë¦¬ì˜¤ 1: ì¼ë°˜ ë ˆì‹œí”¼ ê²€ìƒ‰ ì‹œì‘")
            rewrite_query = state.rewrite.get("text", "")
            return _handle_general_recipe_search(query, rewrite_query)

    except Exception as e:
        logger.error(f"ë ˆì‹œí”¼ ê²€ìƒ‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return {
            "recipe": {"results": [], "ingredients": [], "error": str(e)},
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤, ë ˆì‹œí”¼ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }

# --- ì‹œë‚˜ë¦¬ì˜¤ 1: ì¼ë°˜ ë ˆì‹œí”¼ ê²€ìƒ‰ í•¸ë“¤ëŸ¬ ---
def _handle_general_recipe_search(original_query: str, rewrite_query: str) -> Dict[str, Any]:
    """Tavily APIë¡œ ë ˆì‹œí”¼ë¥¼ ê²€ìƒ‰í•˜ê³  ì‚¬ì´ë“œë°”ì— í‘œì‹œí•  URL ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    # LLM ë˜ëŠ” ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ì— ìµœì í™”ëœ ì¿¼ë¦¬ ìƒì„±
    recipe_query = _extract_recipe_query(original_query, rewrite_query)
    
    # Tavilyë¡œ ì™¸ë¶€ ë ˆì‹œí”¼ ê²€ìƒ‰
    recipe_results = _search_with_tavily(recipe_query)
    
    # í”„ë¡ íŠ¸ì—”ë“œë¡œ ë³´ë‚¼ ìµœì¢… ë©”ì‹œì§€ ìƒì„±
    if recipe_results:
        message = (
            f"{len(recipe_results)}ê°œì˜ ë ˆì‹œí”¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n\n"
            "ğŸ’¡ ì›í•˜ëŠ” ë ˆì‹œí”¼ë¥¼ í´ë¦­í•˜ì—¬ 'ì¬ë£Œ ì¶”ì²œë°›ê¸°' ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ í•„ìš”í•œ ì¬ë£Œë“¤ì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤!"
        )
    else:
        message = "ê´€ë ¨ ë ˆì‹œí”¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."

    return {
        "recipe": {
            "results": recipe_results,      # ì‚¬ì´ë“œë°”ì— í‘œì‹œë  ë ˆì‹œí”¼ URL ëª©ë¡
            "ingredients": [],              # ì´ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” ì¬ë£Œ ëª©ë¡ì´ ë¹„ì–´ìˆìŒ
            "search_query": recipe_query
        },
        "response": message  # chat.jsê°€ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ 'response' í‚¤ ì‚¬ìš©
    }

# --- ì‹œë‚˜ë¦¬ì˜¤ 2: ì„ íƒëœ ë ˆì‹œí”¼ ì¬ë£Œ ì¶”ì²œ í•¸ë“¤ëŸ¬ ---

def _handle_selected_recipe(query: str, state: ChatState = None) -> Dict[str, Any]:
    """ì„ íƒëœ ë ˆì‹œí”¼ URLì„ í¬ë¡¤ë§í•˜ê³ , ì¬ë£Œë¥¼ ì¶”ì¶œí•˜ì—¬ DB ìƒí’ˆê³¼ ë§¤í•‘í•©ë‹ˆë‹¤."""
    
    # ì¿¼ë¦¬ì—ì„œ URL ì¶”ì¶œ
    recipe_url = _extract_recipe_url(query)
    if not recipe_url:
        logger.info("ë ˆì‹œí”¼ URLì„ ì°¾ì§€ ëª»í•¨")
        return {
            "recipe": {"results": [], "ingredients": []},
            "response": "ë ˆì‹œí”¼ URLì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¬ë£Œë¥¼ ì¶”ì²œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }
    
    # URL í¬ë¡¤ë§ ë° LLMì„ í†µí•œ ë‚´ìš© êµ¬ì¡°í™”
    structured_content = _scrape_and_structure_recipe(recipe_url)
    if not structured_content or not structured_content.get("ingredients"):
        logger.info("ë ˆì‹œí”¼ ë‚´ìš©ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŒ")
        return {
            "recipe": {"results": [], "ingredients": []},
            "response": "ë ˆì‹œí”¼ ë‚´ìš©ì„ ë¶„ì„í•  ìˆ˜ ì—†ì–´ ì¬ë£Œ ì¶”ì²œì´ ì–´ë µìŠµë‹ˆë‹¤."
        }
    
    logger.info(f"ë ˆì‹œí”¼ êµ¬ì¡°í™” ì™„ë£Œ: {structured_content.get('title', 'ì œëª© ì—†ìŒ')}")
    
    # ì¶”ì¶œëœ ì¬ë£Œ ëª©ë¡ (ì˜ˆ: ["ì‚¬ê³¼", "ë¼ì§€ê³ ê¸°", "ì–‘íŒŒ"])
    extracted_ingredients = structured_content.get("ingredients", [])
    
    # âœ… ì¶”ê°€: stateì—ì„œ rewrite.keywordsë„ í™œìš©
    additional_keywords = []
    if state and state.rewrite.get("keywords"):
        # ì¬ë£Œ ê´€ë ¨ í‚¤ì›Œë“œë§Œ í•„í„°ë§ (êµ¬ë§¤, ì¬ë£Œ ë“±ì€ ì œì™¸)
        filtered_keywords = [
            k for k in state.rewrite["keywords"] 
            if k not in ['ì¬ë£Œ', 'êµ¬ë§¤', 'ìƒí’ˆ', 'ì¶”ì²œ', 'ì‡¼í•‘ëª°']
        ]
        additional_keywords.extend(filtered_keywords)
    
    # ì¬ë£Œëª…ê³¼ í‚¤ì›Œë“œ í•©ì¹˜ê¸° (ì¤‘ë³µ ì œê±°)
    all_search_terms = list(set(extracted_ingredients + additional_keywords))
    logger.info(f"DB ê²€ìƒ‰ í‚¤ì›Œë“œ: {all_search_terms}")
    
    # ì¬ë£Œëª…ìœ¼ë¡œ DBì˜ ìƒí’ˆ ëª©ë¡ ê²€ìƒ‰ (LIKE ê²€ìƒ‰)
    matched_products = _get_product_details_from_db(all_search_terms)
    
    # AIMessageë¡œ ë³´ì—¬ì¤„ ë ˆì‹œí”¼ ë‚´ìš© í¬ë§·íŒ…
    formatted_recipe_message = _format_recipe_content(structured_content)
    
    logger.info(f"ë ˆì‹œí”¼ ì²˜ë¦¬ ì™„ë£Œ: ì¬ë£Œ {len(all_search_terms)}ê°œ, ì¶”ì²œ ìƒí’ˆ {len(matched_products)}ê°œ")
    print("recipe_search.py matched_products:",matched_products)
    print("recipe_search.py formatted_recipe_message:",formatted_recipe_message)
    print("recipe_search.py structured_content:",structured_content)
    return {
        "recipe": {
            "results": [],
            "ingredients": matched_products,
            "selected_recipe": structured_content
        },
        "meta": {
            "final_message": formatted_recipe_message 
        }
    }

def get_db_connection():
    try:
        return mysql.connector.connect(**DB_CONFIG)
    except Error as e:
        logger.error(f"DB ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

def _get_product_details_from_db(ingredient_names: List[str]) -> List[Dict[str, Any]]:
    """DBì—ì„œ ì¬ë£Œëª…ì„ í¬í•¨(LIKE)í•˜ëŠ” ìƒí’ˆ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    if not ingredient_names:
        return []

    conn = get_db_connection()
    if not conn:
        return []

    try:
        with conn.cursor(dictionary=True) as cursor:
            # ì—¬ëŸ¬ LIKE ì¡°ê±´ì„ ORë¡œ ì—°ê²°í•˜ëŠ” ì¿¼ë¦¬ ìƒì„±
            where_clauses = ' OR '.join(['p.product LIKE %s'] * len(ingredient_names))
            sql = f"""
                SELECT p.product as name, p.unit_price as price, p.origin, p.organic
                FROM product_tbl p
                WHERE {where_clauses}
                LIMIT 15
            """
            
            # LIKE ê²€ìƒ‰ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ìƒì„± (ì˜ˆ: 'ì‚¬ê³¼' -> '%ì‚¬ê³¼%')
            params = [f"%{name}%" for name in ingredient_names]
            
            cursor.execute(sql, params)
            products = cursor.fetchall()

            # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœë¡œ ë°ì´í„° í¬ë§·íŒ…
            formatted_products = []
            for p in products:
                formatted_products.append({
                    'name': p.get('name', ''),
                    'price': float(p.get('price', 0.0)),
                    'origin': p.get('origin', 'ì •ë³´ ì—†ìŒ'),
                    'organic': True if p.get('organic') == 'Y' else False
                })
            return formatted_products
            
    except Error as e:
        logger.error(f"ìƒí’ˆ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []
    finally:
        if conn and conn.is_connected():
            conn.close()

# --- Helper Functions: ì™¸ë¶€ API ë° í¬ë¡¤ë§ ---
def _is_crawlable_url(url: str) -> bool:
    """URLì´ í¬ë¡¤ë§ ê°€ëŠ¥í•œì§€ ê°„ë‹¨íˆ íŒë‹¨í•©ë‹ˆë‹¤."""
    from urllib.parse import urlparse
    
    try:
        parsed = urlparse(url.lower())
        domain = parsed.netloc.replace('www.', '')
        
        # í™•ì‹¤íˆ ì œì™¸í•  ì‚¬ì´íŠ¸ë“¤ (ë™ì˜ìƒ/SNS)
        excluded_patterns = ['youtube.', 'youtu.be', 'instagram.', 'facebook.', 'tiktok.', 'pinterest.']
        if any(pattern in domain for pattern in excluded_patterns):
            return False
        
        # HTML í˜ì´ì§€ì¸ì§€ ê°„ë‹¨ í™•ì¸ (í™•ì¥ì ì²´í¬)
        path = parsed.path.lower()
        if path.endswith(('.mp4', '.avi', '.mov', '.pdf', '.jpg', '.png', '.gif')):
            return False
            
        return True
        
    except Exception:
        return False

def _quick_validate_url(url: str) -> bool:
    """URLì´ ì‹¤ì œë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ ë¹ ë¥´ê²Œ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        import requests
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.head(url, headers=headers, timeout=3)
        
        # 200ëŒ€ ì‘ë‹µì´ê³  HTML ì½˜í…ì¸ ì¸ì§€ í™•ì¸
        if 200 <= response.status_code < 300:
            content_type = response.headers.get('content-type', '').lower()
            return 'text/html' in content_type
            
        return False
    except Exception:
        return False

def _search_with_tavily(query: str) -> List[Dict[str, Any]]:
    """Tavily APIë¡œ ë ˆì‹œí”¼ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. (ê°„ê²°í•œ í•„í„°ë§)"""
    try:
        from tavily import TavilyClient
        client = TavilyClient(api_key=TAVILY_API_KEY)
        
        logger.info(f"Tavily ê²€ìƒ‰ ì‹¤í–‰: '{query}'")
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ì— ì œì™¸ í‚¤ì›Œë“œ ì¶”ê°€
        enhanced_query = f"{query} ë ˆì‹œí”¼ -youtube -instagram -facebook -tiktok"
        
        search_result = client.search(
            query=enhanced_query,
            search_depth="basic",
            max_results=8  # í•„í„°ë§ì„ ê³ ë ¤í•´ ì—¬ìœ ìˆê²Œ
        )
        
        validated_results = []
        
        for res in search_result.get("results", []):
            url = res.get("url", "")
            
            # 1ë‹¨ê³„: ê¸°ë³¸ URL íŒ¨í„´ ê²€ì¦
            if not url or not _is_crawlable_url(url):
                continue
            
            # 2ë‹¨ê³„: ì‹¤ì œ ì ‘ê·¼ ê°€ëŠ¥ì„± ê²€ì¦ (ì²˜ìŒ ëª‡ ê°œë§Œ)
            if len(validated_results) < 5:  # ì²˜ìŒ 5ê°œë§Œ ì‹¤ì œ ê²€ì¦
                if not _quick_validate_url(url):
                    logger.info(f"ì ‘ê·¼ ë¶ˆê°€ëŠ¥í•œ URL ì œì™¸: {url}")
                    continue
            
            validated_results.append({
                "title": res.get("title", "ì œëª© ì—†ìŒ"),
                "url": url,
                "description": res.get("content", "")[:150]
            })
            
            # ì›í•˜ëŠ” ê°œìˆ˜ë§Œí¼ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
            if len(validated_results) >= 3:
                break
        
        logger.info(f"ê²€ì¦ëœ ë ˆì‹œí”¼ URL: {len(validated_results)}ê°œ")
        return validated_results
        
    except Exception as e:
        logger.error(f"Tavily ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def _scrape_and_structure_recipe(url: str) -> Optional[Dict[str, Any]]:
    """URLì„ í¬ë¡¤ë§í•˜ê³  LLMì„ ì‚¬ìš©í•´ ë‚´ìš©ì„ êµ¬ì¡°í™”í•©ë‹ˆë‹¤."""
    logger.info(f"URL í¬ë¡¤ë§ ë° ë¶„ì„ ì‹œì‘: {url}")
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        page_text = soup.get_text(separator='\n', strip=True)
        
        if not openai_client:
            logger.warning("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ë ˆì‹œí”¼ êµ¬ì¡°í™” ë¶ˆê°€.")
            return None
            
        return _llm_extract_recipe_content(page_text[:4000])

    except Exception as e:
        logger.error(f"URL í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì‹¤íŒ¨ {url}: {e}")
        return None

# --- Helper Functions: LLM ì²˜ë¦¬ ---
def _extract_recipe_query(original_query: str, rewrite_query: str = "") -> str:
    """ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ ë ˆì‹œí”¼ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not openai_client:
        return f"{original_query} ë ˆì‹œí”¼"

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ í•µì‹¬ ìš”ë¦¬ ì´ë¦„ë§Œ ì¶”ì¶œí•´ì¤˜. ì˜ˆë¥¼ ë“¤ì–´ 'ê¹€ì¹˜ì°Œê°œ ë§›ìˆê²Œ ë“ì´ëŠ” ë²• ì•Œë ¤ì¤˜' -> 'ê¹€ì¹˜ì°Œê°œ'"},
                {"role": "user", "content": f"ì›ë³¸: '{original_query}', ì¬ì‘ì„±: '{rewrite_query}'"}
            ],
            temperature=0.1, max_tokens=50
        )
        return response.choices[0].message.content.strip().strip('"')
    except Exception as e:
        logger.error(f"LLM ì¿¼ë¦¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return f"{original_query} ë ˆì‹œí”¼"

def _llm_extract_recipe_content(page_text: str) -> Dict[str, Any]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ë ˆì‹œí”¼ ì •ë³´ë¥¼ JSON í˜•íƒœë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤."""
    system_prompt = """ë‹¹ì‹ ì€ ì‹ ì„ ì‹í’ˆ ì‡¼í•‘ëª°ì„ ìœ„í•œ ë ˆì‹œí”¼ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ë ˆì‹œí”¼ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ê³ ê°ì—ê²Œ í•„ìš”í•œ ì¬ë£Œë¥¼ ì¶”ì²œí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ì„¸ìš”.

**ì¶”ì¶œ ê·œì¹™:**
1. **title**: ìš”ë¦¬ì˜ ì •í™•í•œ ì´ë¦„ (ì˜ˆ: "ê¹€ì¹˜ì°Œê°œ", "ìˆ˜ë°•í™”ì±„")
2. **ingredients**: ì‡¼í•‘ëª°ì—ì„œ êµ¬ë§¤ ê°€ëŠ¥í•œ ì‹ ì„ ì‹í’ˆ ì¬ë£Œë§Œ ì¶”ì¶œ
   - ê¸°ë³¸ ëª…ì‚¬ í˜•íƒœë¡œë§Œ ì¶”ì¶œ: 'ìˆ˜ë°•', 'ë¼ì§€ê³ ê¸°', 'ì–‘íŒŒ'
   - ì–‘ë…/ì¡°ë¯¸ë£ŒëŠ” í¬í•¨í•˜ë˜ ë‹¨ìœ„/ìˆ˜ëŸ‰ ì œì™¸: 'ê°„ì¥', 'ì°¸ê¸°ë¦„'
   - ê°€ê³µì‹í’ˆì€ êµ¬ë§¤ ê°€ëŠ¥í•˜ë©´ í¬í•¨: 'ë‘ë¶€', 'ë©´'
   - ì œì™¸í•  ê²ƒ: ë¬¼, ì†Œê¸ˆ, í›„ì¶”, ì„¤íƒ• ë“± ê¸°ë³¸ ì–‘ë…
3. **instructions**: ê³ ê°ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì¡°ë¦¬ë²• ìš”ì•½
   - í•µì‹¬ ë‹¨ê³„ë§Œ ê°„ë‹¨íˆ ì •ë¦¬
   - ê° ë‹¨ê³„ëŠ” '\n'ìœ¼ë¡œ êµ¬ë¶„
   - ì „ë¬¸ ìš©ì–´ë³´ë‹¤ëŠ” ì¼ë°˜ì ì¸ í‘œí˜„ ì‚¬ìš©

**ì¶œë ¥ í˜•ì‹:**
ë°˜ë“œì‹œ ë‹¤ìŒ JSON êµ¬ì¡°ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
```json
{
    "title": "ìš”ë¦¬ëª…",
    "ingredients": ["ì¬ë£Œ1", "ì¬ë£Œ2", "ì¬ë£Œ3"],
    "instructions": "1ë‹¨ê³„ ì„¤ëª…\n2ë‹¨ê³„ ì„¤ëª…\n3ë‹¨ê³„ ì„¤ëª…"
}
```

**ì˜ˆì‹œ:**
ì…ë ¥: "ë¼ì§€ê³ ê¸° ê¹€ì¹˜ì°Œê°œ ë ˆì‹œí”¼... ë¼ì§€ê³ ê¸° 200g, ê¹€ì¹˜ 300g, ì–‘íŒŒ 1ê°œ, ëŒ€íŒŒ 2ëŒ€, ë‘ë¶€ 1ëª¨..."
ì¶œë ¥:
```json
{
    "title": "ë¼ì§€ê³ ê¸° ê¹€ì¹˜ì°Œê°œ",
    "ingredients": ["ë¼ì§€ê³ ê¸°", "ê¹€ì¹˜", "ì–‘íŒŒ", "ëŒ€íŒŒ", "ë‘ë¶€", "ê³ ì¶§ê°€ë£¨", "ê°„ì¥"],
    "instructions": "1. ë¼ì§€ê³ ê¸°ë¥¼ ë¨¼ì € ë³¶ì•„ ê¸°ë¦„ì„ ë‚¸ë‹¤\n2. ê¹€ì¹˜ì™€ ì–‘íŒŒë¥¼ ë„£ê³  í•¨ê»˜ ë³¶ëŠ”ë‹¤\n3. ë¬¼ì„ ë¶€ì–´ ë“ì¸ í›„ ë‘ë¶€ì™€ ëŒ€íŒŒë¥¼ ë„£ëŠ”ë‹¤\n4. ê°„ì¥ê³¼ ê³ ì¶§ê°€ë£¨ë¡œ ê°„ì„ ë§ì¶˜ë‹¤"
}
```

ì¤‘ìš”: JSON í˜•ì‹ ì™¸ì— ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”."""
    user_prompt = f"ë‹¤ìŒ ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ë ˆì‹œí”¼ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì¤˜:\n\n---\n{page_text}\n---"

    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.2, max_tokens=1024
    )
    
    try:
        content = json.loads(response.choices[0].message.content)
        if isinstance(content.get("ingredients"), str):
            content["ingredients"] = [item.strip() for item in content["ingredients"].split(',')]
        return content
    except (json.JSONDecodeError, AttributeError) as e:
        logger.error(f"LLM JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return {}

def _extract_recipe_url(query: str) -> Optional[str]:
    """ì¿¼ë¦¬ ë¬¸ìì—´ì—ì„œ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    match = re.search(r'URL:\s*(https?://[^\s]+)', query)
    if match:
        return match.group(1)
    logger.warning(f"ì¿¼ë¦¬ì—ì„œ URLì„ ì°¾ì§€ ëª»í•¨: {query}")
    return None

def _format_recipe_content(structured_content: Dict[str, Any]) -> str:
    """êµ¬ì¡°í™”ëœ ë ˆì‹œí”¼ ë°ì´í„°ë¥¼ AIMessageì— í‘œì‹œí•  ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    title = structured_content.get("title", "ë ˆì‹œí”¼ ì •ë³´")
    ingredients = structured_content.get("ingredients", [])
    instructions = structured_content.get("instructions", "ì¡°ë¦¬ë²• ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    ingredients_text = "\n".join(f"- {ing}" for ing in ingredients[:10])
    if len(ingredients) > 10:
        ingredients_text += "\n- ë“±..."

    formatted_message = (
        f"**{title}**\n\n"
        f"**í•„ìš”í•œ ì¬ë£Œ:**\n{ingredients_text}\n\n"
        f"**ì¡°ë¦¬ë²• ìš”ì•½:**\n{instructions}\n\n"
        "---\n"
        "**ìš°ì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ ì¶”ì²œ ì¬ë£Œë“¤ì„ ë°”ë¡œ ì¥ë°”êµ¬ë‹ˆì— ë‹´ì•„ë³´ì„¸ìš”!**"
    )
    
    return formatted_message