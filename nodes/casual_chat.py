"""
casual_chat.py - ì¼ìƒëŒ€í™” ì²˜ë¦¬ ë…¸ë“œ

ì‚¬ìš©ìì˜ ì¼ìƒì ì¸ ì¸ì‚¬ë‚˜ ëŒ€í™”ì— ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.
ì ì ˆí•œ ê²½ìš° ì‡¼í•‘ëª°ì˜ ê¸°ëŠ¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•©ë‹ˆë‹¤.
"""

import logging
import random
from typing import Dict, Any, List
import os
import sys

# ìƒëŒ€ ê²½ë¡œë¡œ graph_interfaces ì„í¬íŠ¸
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from graph_interfaces import ChatState

# íˆìŠ¤í† ë¦¬ ë° ê°ì • ë¶„ì„ ìœ í‹¸ ì„í¬íŠ¸
from utils.chat_history import (
    analyze_user_emotion,
    get_empathy_response,
    recommend_food_by_emotion,
    update_user_context,
    get_recent_context,
    get_contextual_analysis
)

logger = logging.getLogger("CASUAL_CHAT")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
try:
    import openai
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if openai_api_key:
        openai_client = openai.OpenAI(api_key=openai_api_key)
    else:
        openai_client = None
        logger.warning("OpenAI API key not found. Using predefined responses.")
except ImportError:
    openai_client = None
    logger.warning("OpenAI package not available.")

def casual_chat(state: ChatState) -> ChatState:
    """íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ë§¥ë½ì  ì¼ìƒëŒ€í™” ì²˜ë¦¬ í•¨ìˆ˜"""
    logger.info(f"ì¼ìƒëŒ€í™” ì²˜ë¦¬: {state.query}")

    try:
        # 1. íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ê°ì • ë¶„ì„
        emotion_analysis = analyze_user_emotion(state.query, state.conversation_history)
        logger.info(f"ê°ì • ë¶„ì„ ê²°ê³¼: {emotion_analysis}")

        # 2. ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        update_user_context(state, emotion_analysis)

        # 3. ê³µê° ë©”ì‹œì§€ ìƒì„±
        empathy_msg = get_empathy_response(
            emotion_analysis["primary_emotion"],
            emotion_analysis["context"],
            emotion_analysis.get("intensity", "medium")
        )
        logger.info(f"ê³µê° ë©”ì‹œì§€: {empathy_msg}")

        # 4. ì‚¬ìš©ì ì„ í˜¸ë„ ê°€ì ¸ì˜¤ê¸°
        user_preferences = {}
        if hasattr(state, 'user_id') and state.user_id:
            try:
                from policy import get_user_preferences
                user_preferences = get_user_preferences(state.user_id)
                logger.info(f"ì‚¬ìš©ì {state.user_id} ì„ í˜¸ë„: {user_preferences}")
            except Exception as e:
                logger.warning(f"ì‚¬ìš©ì ì„ í˜¸ë„ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # 5. ìŒì‹ ì¶”ì²œ ìƒì„± (ì„ í˜¸ë„ ê³ ë ¤)
        food_rec = recommend_food_by_emotion(
            emotion_analysis["primary_emotion"],
            emotion_analysis["context"],
            emotion_analysis.get("food_mood", "")
        )

        # ì„ í˜¸ë„ ì •ë³´ë¥¼ food_recì— ì¶”ê°€
        if user_preferences:
            food_rec["user_preferences"] = user_preferences

        logger.info(f"ìŒì‹ ì¶”ì²œ: {food_rec}")

        # 5. í†µí•© ì‘ë‹µ êµ¬ì„±
        if openai_client:
            response = generate_contextual_response_llm(
                state.query, state.conversation_history,
                emotion_analysis, empathy_msg, food_rec
            )
        else:
            response = generate_contextual_response_fallback(
                empathy_msg, food_rec
            )

        state.response = response
        logger.info(f"ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ: {response[:100]}...")

    except Exception as e:
        logger.error(f"ì¼ìƒëŒ€í™” ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        if openai_client:
            state.response = generate_fallback_response_llm(state.query)
        else:
            state.response = "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"

    return state

# def casual_chat(state: ChatState) -> ChatState:
#     """íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ë§¥ë½ì  ì¼ìƒëŒ€í™” ì²˜ë¦¬ í•¨ìˆ˜"""
#     logger.info(f"ì¼ìƒëŒ€í™” ì²˜ë¦¬: {state.query}")

#     try:
#         # [NEW] íˆìŠ¤í† ë¦¬ ì•ˆì „ í™•ë³´
#         history: List[Dict] = getattr(state, "conversation_history", None) or []
#         if not isinstance(history, list):
#             history = []

#         # 1. íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ê°ì • ë¶„ì„
#         emotion_analysis = analyze_user_emotion(state.query, history)
#         logger.info(f"ê°ì • ë¶„ì„ ê²°ê³¼: {emotion_analysis}")

#         # 2. ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
#         update_user_context(state, emotion_analysis)

#         # 3. ê³µê° ë©”ì‹œì§€ ìƒì„±
#         empathy_msg = get_empathy_response(
#             emotion_analysis["primary_emotion"],
#             emotion_analysis["context"],
#             emotion_analysis.get("intensity", "medium")
#         )
#         logger.info(f"ê³µê° ë©”ì‹œì§€: {empathy_msg}")

#         # 4. ìŒì‹ ì¶”ì²œ ìƒì„±
#         food_rec = recommend_food_by_emotion(
#             emotion_analysis["primary_emotion"],
#             emotion_analysis["context"],
#             emotion_analysis.get("food_mood", "")
#         )
#         logger.info(f"ìŒì‹ ì¶”ì²œ: {food_rec}")

#         # 5. í†µí•© ì‘ë‹µ êµ¬ì„±
#         if openai_client:
#             response = generate_contextual_response_llm(
#                 state.query, history, emotion_analysis, empathy_msg, food_rec
#             )
#         else:
#             response = generate_contextual_response_fallback(
#                 empathy_msg, food_rec
#             )

#         # [NEW] íˆìŠ¤í† ë¦¬ê°€ ì—†ì„ ë•Œ 'ê¸°ì–µ' ì„œìˆ  ì°¨ë‹¨
#         response = _post_guard_filter(response, history)

#         state.response = response
#         logger.info(f"ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ: {response[:100]}...")

#     except Exception as e:
#         logger.error(f"ì¼ìƒëŒ€í™” ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
#         if openai_client:
#             state.response = generate_fallback_response_llm(state.query)
#         else:
#             state.response = "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"

#     return state



def generate_contextual_response_llm(query: str, history: List[Dict], emotion_analysis: Dict, empathy_msg: str, food_rec: Dict) -> str:
    """LLM ê¸°ë°˜ ë§¥ë½ì  ìµœì¢… ì‘ë‹µ ìƒì„±"""
    try:
        # í–¥ìƒëœ ë§¥ë½ ë¶„ì„ ì‚¬ìš©
        contextual_analysis = get_contextual_analysis(history, query)
        recent_context = get_recent_context(history, turns=3) if history else "ìƒˆë¡œìš´ ëŒ€í™”"

        # íˆìŠ¤í† ë¦¬ ìƒíƒœ í™•ì¸
        is_empty_history = recent_context == "ìƒˆë¡œìš´ ëŒ€í™”"

        system_prompt = f"""
ë‹¹ì‹ ì€ ì‹ ì„ ì‹í’ˆ ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì˜ ë”°ëœ»í•˜ê³  ê³µê°ì ì¸ ì±—ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê°ì •ê³¼ ìƒí™©ì„ ì´í•´í•˜ê³ , ìì—°ìŠ¤ëŸ½ê³  ë§¥ë½ì ì¸ ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”.

ğŸš¨ **ì ˆëŒ€ ê·œì¹™**: ëŒ€í™” ë§¥ë½ì´ "ìƒˆë¡œìš´ ëŒ€í™”"ì´ë©´ ì´ì „ ì¶”ì²œì„ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”!

ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
1. ì‚¬ìš©ìì˜ ê°ì •ì— ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ê¸°
2. ìƒí™©ì— ë§ëŠ” ìŒì‹ì´ë‚˜ ë ˆì‹œí”¼ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ì²œ
3. **ì‚¬ìš©ìì˜ ì•ŒëŸ¬ì§€/ë¹„ì„ í˜¸ ìŒì‹ì„ ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ ê²ƒ**
4. ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ í†¤ ìœ ì§€
5. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ë˜ ë”°ëœ»í•˜ê²Œ
6. ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš© (ê³¼í•˜ì§€ ì•Šê²Œ)
7. ì‡¼í•‘ëª° ì„œë¹„ìŠ¤ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
8. ëŒ€í™”ë¥¼ ì´ì–´ê°ˆ ìˆ˜ ìˆëŠ” ì—´ë¦° ì§ˆë¬¸ í¬í•¨

{"ë§¥ë½ ì—°ì†ì„± ê·œì¹™ (ê¸°ì¡´ ëŒ€í™”ê°€ ìˆì„ ë•Œë§Œ):" if not is_empty_history else "ìƒˆë¡œìš´ ëŒ€í™” ê·œì¹™:"}
{"- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì—°ì†ì„± ìˆëŠ” ì‘ë‹µ" if not is_empty_history else "- ì´ì „ ì¶”ì²œì„ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ê³  í˜„ì¬ ìƒí™©ì—ë§Œ ì§‘ì¤‘"}
{"- ì´ì „ ì¶”ì²œê³¼ ë‹¤ë¥¸ ìƒˆë¡œìš´ ì˜µì…˜ ì œì•ˆ" if not is_empty_history else "- 'ì•„ê¹Œ', 'ì´ì „ì—', 'ë°©ê¸ˆ' ë“±ì˜ ê³¼ê±° ì§€ì¹­ í‘œí˜„ ê¸ˆì§€"}
{"- ì‚¬ìš©ìì˜ ë³€í™”í•˜ëŠ” ì·¨í–¥ ë°˜ì˜" if not is_empty_history else "- í˜„ì¬ ê°ì •ê³¼ ìƒí™©ì— ê¸°ë°˜í•œ ìƒˆë¡œìš´ ì¶”ì²œë§Œ"}

ì¤‘ìš”:
1. ê³µê° ë©”ì‹œì§€ì™€ ìŒì‹ ì¶”ì²œì„ ìì—°ìŠ¤ëŸ½ê²Œ ìœµí•©ëœ ì‘ë‹µìœ¼ë¡œ ë§Œë“¤ê¸°
{"2. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì–¸ê¸‰í•˜ì—¬ ì—°ì†ì„± í™•ë³´í•˜ê¸°" if not is_empty_history else "2. í˜„ì¬ ìƒí™©ì—ë§Œ ì§‘ì¤‘í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ìƒˆë¡œìš´ ì¶”ì²œí•˜ê¸°"}
3. ì‚¬ìš©ìê°€ ê³„ì† ëŒ€í™”í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ì´ê³  ê´€ë ¨ì„± ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ê¸°
"""

        user_prompt = f"""
ëŒ€í™” ìƒíƒœ: {"ìƒˆë¡œìš´ ëŒ€í™” (íˆìŠ¤í† ë¦¬ ì—†ìŒ)" if is_empty_history else "ê¸°ì¡´ ëŒ€í™” ì§„í–‰ ì¤‘"}

ìµœê·¼ ëŒ€í™” ë§¥ë½:
{recent_context}

í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€: "{query}"

ë§¥ë½ ë¶„ì„ ê²°ê³¼:
- ì´ì „ ì¶”ì²œ ë‚´ì—­: {', '.join(contextual_analysis.get('previous_recommendations', [])) or 'ì—†ìŒ'}
- í›„ì† ì˜ë„: {contextual_analysis.get('followup_intent', 'none')}
- ì œì•ˆ ëŒ€ì•ˆ: {', '.join(contextual_analysis.get('suggested_alternatives', [])) or 'ì—†ìŒ'}
- ë§¥ë½ ìš”ì•½: {contextual_analysis.get('context_summary', 'ìƒˆë¡œìš´ ëŒ€í™”')}

ê°ì • ë¶„ì„ ê²°ê³¼:
- ì£¼ìš” ê°ì •: {emotion_analysis.get('primary_emotion', 'neutral')}
- ìƒí™©: {emotion_analysis.get('context', 'ì¼ìƒ ëŒ€í™”')}
- ê°•ë„: {emotion_analysis.get('intensity', 'medium')}

ê³µê° ë©”ì‹œì§€ ì°¸ê³ : {empathy_msg}

ìŒì‹ ì¶”ì²œ ì •ë³´:
- ì¶”ì²œ ìŒì‹: {', '.join(food_rec.get('keywords', ['ë§›ìˆëŠ” ìŒì‹'])[:3])}
- ì¶”ì²œ ì´ìœ : {food_rec.get('reason', 'ê¸°ë¶„ ì „í™˜ì— ë„ì›€ì´ ë  ê±°ì˜ˆìš”')}

ì‚¬ìš©ì ì„ í˜¸ë„ ì •ë³´:
{f"- ì•ŒëŸ¬ì§€: {food_rec.get('user_preferences', {}).get('allergy', 'ì—†ìŒ')}" if food_rec.get('user_preferences') else "- ì„ í˜¸ë„ ì •ë³´ ì—†ìŒ"}
{f"- ë¹„ì„ í˜¸ ìŒì‹: {food_rec.get('user_preferences', {}).get('unfavorite', 'ì—†ìŒ')}" if food_rec.get('user_preferences') else ""}
{f"- ë¹„ê±´ ì—¬ë¶€: {'ì˜ˆ' if food_rec.get('user_preferences', {}).get('vegan') else 'ì•„ë‹ˆì˜¤'}" if food_rec.get('user_preferences') else ""}

{"âš ï¸ ì¤‘ìš”: ëŒ€í™” ìƒíƒœê°€ 'ìƒˆë¡œìš´ ëŒ€í™”'ì´ë¯€ë¡œ ì´ì „ ì¶”ì²œì„ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”!" if is_empty_history else ""}
âš ï¸ ì„ í˜¸ë„ ì¤€ìˆ˜: ì‚¬ìš©ìì˜ ì•ŒëŸ¬ì§€ë‚˜ ë¹„ì„ í˜¸ ìŒì‹ì´ ìˆë‹¤ë©´ ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”!

ìœ„ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•´ì„œ ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ì‘ë‹µì„ ìƒì„±í•´ì£¼ì„¸ìš”.
{"í˜„ì¬ ìƒí™©ê³¼ ê°ì •ì—ë§Œ ê¸°ë°˜í•œ ìƒˆë¡œìš´ ì¶”ì²œì„ í•´ì£¼ì„¸ìš”." if is_empty_history else "í›„ì† ì˜ë„ê°€ 'alternative'ì¸ ê²½ìš° ì´ì „ ì¶”ì²œì„ ì–¸ê¸‰í•˜ë©° ìƒˆë¡œìš´ ëŒ€ì•ˆì„ ì œì‹œí•˜ì„¸ìš”."}
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=200,
            temperature=0.7
        )

        result = response.choices[0].message.content.strip()
        logger.info(f"LLM ë§¥ë½ì  ì‘ë‹µ ìƒì„±: {result}")
        return result

    except Exception as e:
        logger.error(f"LLM ë§¥ë½ì  ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return generate_contextual_response_fallback(empathy_msg, food_rec)


# def generate_contextual_response_llm(query: str, history: List[Dict], emotion_analysis: Dict, empathy_msg: str, food_rec: Dict) -> str:
#     """LLM ê¸°ë°˜ ë§¥ë½ì  ìµœì¢… ì‘ë‹µ ìƒì„±"""
#     try:
#         contextual_analysis = get_contextual_analysis(history, query)

#         # [NEW] íˆìŠ¤í† ë¦¬ ì§ë ¬í™” (ëª¨ë¸ì´ ë³¼ ì›ì²œ ì¦ê±°)
#         def _fmt(m):
#             role = m.get("role", "user")
#             content = m.get("content", "")
#             return f"{role}: {content}"
#         history_text = "\n".join(_fmt(m) for m in history[-12:]) or "(ë¹„ì–´ ìˆìŒ)"

#         recent_context = get_recent_context(history, turns=3) if history else "ìƒˆë¡œìš´ ëŒ€í™”"

#         system_prompt = """
# ë‹¹ì‹ ì€ ì‹ ì„ ì‹í’ˆ ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì˜ ë”°ëœ»í•˜ê³  ê³µê°ì ì¸ ì±—ë´‡ì…ë‹ˆë‹¤.

# ì•„ë˜ **ì ˆëŒ€ ê·œì¹™**ì„ ë°˜ë“œì‹œ ì§€í‚¤ì„¸ìš”.
# 1) ì˜¤ì§ <HISTORY>ì— ìˆëŠ” ë‚´ìš©ë§Œ 'ì´ì „ì— ëŒ€í™”í–ˆë‹¤/ì¶”ì²œí–ˆë‹¤'ê³  ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# 2) <HISTORY>ì— ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ê³¼ê±°ë¥¼ ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.
# 3) <HISTORY>ì— ì´ì „ ì¶”ì²œì´ ì—†ì„ ë•ŒëŠ”, 'ê¸°ë¡ì„ ì°¾ì§€ ëª»í–ˆë‹¤'ê³  ì†”ì§íˆ ë§í•˜ê³  ë‹¤ì‹œ ë¬¼ì–´ë´…ë‹ˆë‹¤.
# 4) ê³µê° â†’ í•„ìš”í•œ ê²½ìš° ê°„ë‹¨ ì¶”ì²œ â†’ ì—´ë¦° ì§ˆë¬¸ ìˆœìœ¼ë¡œ 2~3ë¬¸ì¥ ë‚´ì— ë‹µí•˜ì„¸ìš”. (ê³¼ì¥ ê¸ˆì§€, ì´ëª¨ì§€ ê³¼ë‹¤ ê¸ˆì§€)
# """

#         user_prompt = f"""
# <HISTORY>
# {history_text}
# </HISTORY>

# ìµœê·¼ ëŒ€í™” ë§¥ë½: {recent_context}
# í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€: "{query}"

# ë§¥ë½ ë¶„ì„:
# - ì´ì „ ì¶”ì²œ ë‚´ì—­: {', '.join(contextual_analysis.get('previous_recommendations', [])) or 'ì—†ìŒ'}
# - í›„ì† ì˜ë„: {contextual_analysis.get('followup_intent', 'none')}
# - ì œì•ˆ ëŒ€ì•ˆ: {', '.join(contextual_analysis.get('suggested_alternatives', [])) or 'ì—†ìŒ'}
# - ë§¥ë½ ìš”ì•½: {contextual_analysis.get('context_summary', 'ìƒˆë¡œìš´ ëŒ€í™”')}

# ê°ì • ë¶„ì„:
# - ì£¼ìš” ê°ì •: {emotion_analysis.get('primary_emotion', 'neutral')}
# - ìƒí™©: {emotion_analysis.get('context', 'ì¼ìƒ ëŒ€í™”')}
# - ê°•ë„: {emotion_analysis.get('intensity', 'medium')}

# ê³µê° ë©”ì‹œì§€(ì°¸ê³ ): {empathy_msg}

# ìŒì‹ ì¶”ì²œ(ì°¸ê³ ):
# - í‚¤ì›Œë“œ: {', '.join(food_rec.get('keywords', ['ë§›ìˆëŠ” ìŒì‹'])[:3])}
# - ì´ìœ : {food_rec.get('reason', 'ê¸°ë¶„ ì „í™˜ì— ë„ì›€ì´ ë  ê±°ì˜ˆìš”')}

# ì£¼ì˜:
# - <HISTORY>ì— 'ì¹¼êµ­ìˆ˜' ë“± ê³¼ê±° ìš”ì²­/ì¶”ì²œì´ **ì—†ìœ¼ë©´** ì ˆëŒ€ 'ì•„ê¹Œ ~ì¶”ì²œë“œë ¸ëŠ”ë°' ê°™ì€ í‘œí˜„ì„ ì“°ì§€ ë§ˆì„¸ìš”.
# - ê³¼ê±°ê°€ ì—†ìœ¼ë©´ "ê¸°ë¡ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì–´ë–¤ ê±¸ ì°¾ê³  ê³„ì…¨ëŠ”ì§€ ì•Œë ¤ì£¼ì‹¤ë˜ìš”?"ì²˜ëŸ¼ ì •ì§í•˜ê²Œ í™•ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.
# """

#         response = openai_client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": user_prompt}
#             ],
#             max_tokens=200,
#             temperature=0.7
#         )

#         result = response.choices[0].message.content.strip()
#         logger.info(f"LLM ë§¥ë½ì  ì‘ë‹µ ìƒì„±: {result}")
#         return result

#     except Exception as e:
#         logger.error(f"LLM ë§¥ë½ì  ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
#         return generate_contextual_response_fallback(empathy_msg, food_rec)



def generate_contextual_response_fallback(empathy_msg: str, food_rec: Dict) -> str:
    """í´ë°± ë§¥ë½ì  ì‘ë‹µ ìƒì„±"""
    food_keywords = food_rec.get('keywords', ['ë§›ìˆëŠ” ìŒì‹'])[:3]
    food_reason = food_rec.get('reason', 'ê¸°ë¶„ ì „í™˜ì— ë„ì›€ì´ ë  ê±°ì˜ˆìš”')

    return f"{empathy_msg}\n\n{food_reason} {', '.join(food_keywords)} ì–´ë– ì„¸ìš”?"

# def generate_contextual_response_fallback(empathy_msg: str, food_rec: Dict) -> str:
#     """í´ë°± ë§¥ë½ì  ì‘ë‹µ ìƒì„±"""
#     food_keywords = food_rec.get('keywords', ['ë§›ìˆëŠ” ìŒì‹'])[:3]
#     food_reason = food_rec.get('reason', 'ê¸°ë¶„ ì „í™˜ì— ë„ì›€ì´ ë  ê±°ì˜ˆìš”')
#     return f"{empathy_msg}\n\n{food_reason} {', '.join(food_keywords)} ì–´ë– ì„¸ìš”?"


def generate_fallback_response_llm(query: str) -> str:
    """LLM ê¸°ë°˜ ê¸°ë³¸ í´ë°± ì‘ë‹µ"""
    try:
        system_prompt = """
ë‹¹ì‹ ì€ ì‹ ì„ ì‹í’ˆ ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì˜ ì¹œê·¼í•œ ì±—ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì—ê²Œ ë”°ëœ»í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì‘ë‹µì„ í•´ì£¼ì„¸ìš”.
1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ë˜ ì¹œê·¼í•œ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ],
            max_tokens=100,
            temperature=0.6
        )

        return response.choices[0].message.content.strip()

    except Exception as e:
        logger.error(f"LLM í´ë°± ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"


# ê¸°ì¡´ í•¨ìˆ˜ë“¤ ë°±ì—… (ì°¸ê³ ìš©)
def casual_chat_legacy(state: ChatState) -> ChatState:
    """ê¸°ì¡´ ì¼ìƒëŒ€í™” ì²˜ë¦¬ í•¨ìˆ˜ (ë°±ì—…)"""
    logger.info(f"ì¼ìƒëŒ€í™” ì²˜ë¦¬: {state.query}")

    try:
        if openai_client:
            response = _generate_llm_response(state.query)
        else:
            response = _get_predefined_response(state.query)

        state.response = response
        logger.info(f"ì¼ìƒëŒ€í™” ì‘ë‹µ ìƒì„± ì™„ë£Œ: {response[:50]}...")

    except Exception as e:
        logger.error(f"ì¼ìƒëŒ€í™” ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        state.response = "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ í•˜ë£¨ë„ ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”. ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"

    return state

def _generate_llm_response(query: str) -> str:
    """LLMì„ ì‚¬ìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„±"""

    system_prompt = """
    ë‹¹ì‹ ì€ ì‹ ì„ ì‹í’ˆ ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì˜ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì¼ìƒì ì¸ ëŒ€í™”ì— ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”.

    ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
    1. ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ í†¤ìœ¼ë¡œ ì‘ë‹µ
    2. ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš© (ê³¼í•˜ì§€ ì•Šê²Œ)
    3. ìƒí™©ì— ë”°ë¼ ì‡¼í•‘ëª°ì˜ ì„œë¹„ìŠ¤ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
    4. ì‘ë‹µì€ 1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ

    ì˜ˆì‹œ:
    - "ì•ˆë…•" â†’ "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ê³  ê³„ì‹ ê°€ìš”? ì˜¤ëŠ˜ í•„ìš”í•œ ì‹ ì„ í•œ ì‹ì¬ë£Œê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!"
    - "ê³ ë§ˆì›Œ" â†’ "ì²œë§Œì—ìš”! ğŸ˜Š ë„ì›€ì´ ë˜ì–´ì„œ ê¸°ë»ìš”. ë˜ í•„ìš”í•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”."
    - "ì˜ ì§€ë‚´?" â†’ "ì €ëŠ” í•­ìƒ ê±´ê°•í•˜ê²Œ ì˜ ì§€ë‚´ê³  ìˆì–´ìš”! ğŸ˜Š ê³ ê°ë‹˜ì€ ì–´ë– ì‹ ê°€ìš”? ì˜¤ëŠ˜ ë§›ìˆëŠ” ìš”ë¦¬ ê³„íšì´ ìˆìœ¼ì‹œë‚˜ìš”?"
    """

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ],
            max_tokens=150,
            temperature=0.7
        )

        return response.choices[0].message.content.strip()

    except Exception as e:
        logger.error(f"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        return _get_predefined_response(query)

def _get_predefined_response(query: str) -> str:
    """ë¯¸ë¦¬ ì •ì˜ëœ ì‘ë‹µ íŒ¨í„´ ë§¤ì¹­"""

    query_lower = query.lower()

    # ì¸ì‚¬ë§ ì‘ë‹µ
    greeting_patterns = {
        "ì•ˆë…•": [
            "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ê³  ê³„ì‹ ê°€ìš”? ì˜¤ëŠ˜ í•„ìš”í•œ ì‹ ì„ í•œ ì‹ì¬ë£Œê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!",
            "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ë°˜ê°€ì›Œìš”! ì˜¤ëŠ˜ ì–´ë–¤ ë§›ìˆëŠ” ìš”ë¦¬ë¥¼ ê³„íší•˜ê³  ê³„ì‹ ê°€ìš”?",
            "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì¢‹ì€ í•˜ë£¨ì˜ˆìš”! ì‹ ì„ í•œ ì¬ë£Œë‚˜ ë§›ìˆëŠ” ë ˆì‹œí”¼ê°€ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë„ì™€ë“œë¦´ê²Œìš”!"
        ],
        "ì¢‹ì€": [
            "ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”! ğŸ˜Š ì˜¤ëŠ˜ë„ ì‹ ì„ í•˜ê³  ë§›ìˆëŠ” ì‹ì¬ë£Œë¡œ ê±´ê°•í•œ ì‹ì‚¬ ì¤€ë¹„í•´ ë³´ì„¸ìš”!",
            "ê°ì‚¬í•´ìš”! ğŸ˜Š ê³ ê°ë‹˜ë„ ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”! ë§›ìˆëŠ” ìš”ë¦¬ ê³„íšì´ ìˆìœ¼ì‹œë‚˜ìš”?"
        ],
        "ì•„ì¹¨": [
            "ì¢‹ì€ ì•„ì¹¨ì´ì—ìš”! ğŸŒ… ì˜¤ëŠ˜ ì•„ì¹¨ ì‹ì‚¬ëŠ” ì¤€ë¹„í•˜ì…¨ë‚˜ìš”? ì‹ ì„ í•œ ì¬ë£Œë¡œ ê±´ê°•í•œ í•˜ë£¨ë¥¼ ì‹œì‘í•´ ë³´ì„¸ìš”!",
            "ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤! ğŸ˜Š ì˜¤ëŠ˜ í•˜ë£¨ë„ ë§›ìˆê³  ê±´ê°•í•œ ì‹ì‚¬ë¡œ í™œê¸°ì°¨ê²Œ ë³´ë‚´ì„¸ìš”!"
        ]
    }

    # ê°ì‚¬ í‘œí˜„ ì‘ë‹µ
    gratitude_patterns = {
        "ê³ ë§ˆì›Œ": [
            "ì²œë§Œì—ìš”! ğŸ˜Š ë„ì›€ì´ ë˜ì–´ì„œ ê¸°ë»ìš”. ë˜ í•„ìš”í•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!",
            "ë³„ë§ì”€ì„ìš”! ğŸ˜Š ì–¸ì œë“  ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë¶ˆëŸ¬ì£¼ì„¸ìš”!"
        ],
        "ê°ì‚¬": [
            "ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ˜Š ì €í¬ë„ ê³ ê°ë‹˜ê»˜ ë„ì›€ì´ ë  ìˆ˜ ìˆì–´ì„œ ê¸°ë»ìš”!",
            "ê³ ë§ˆìš´ ë§ì”€ì´ì—ìš”! ğŸ˜Š ì•ìœ¼ë¡œë„ ë” ì¢‹ì€ ì„œë¹„ìŠ¤ë¡œ ë³´ë‹µí•˜ê² ìŠµë‹ˆë‹¤!"
        ]
    }

    # ì•ˆë¶€ ë¬¸ì˜ ì‘ë‹µ
    wellbeing_patterns = {
        "ì˜": [
            "ì €ëŠ” í•­ìƒ ê±´ê°•í•˜ê²Œ ì˜ ì§€ë‚´ê³  ìˆì–´ìš”! ğŸ˜Š ê³ ê°ë‹˜ì€ ì–´ë– ì‹ ê°€ìš”? ì˜¤ëŠ˜ ë§›ìˆëŠ” ìš”ë¦¬ ê³„íšì´ ìˆìœ¼ì‹œë‚˜ìš”?",
            "ë„¤, ì˜ ì§€ë‚´ê³  ìˆì–´ìš”! ğŸ˜Š ê³ ê°ë‹˜ê»˜ì„œë„ ê±´ê°•í•˜ê²Œ ì§€ë‚´ì‹œê¸¸ ë°”ë¼ìš”! ì˜ì–‘ê°€ ìˆëŠ” ì‹ì‚¬ ë„ì™€ë“œë¦´ê¹Œìš”?"
        ]
    }

    # ë‚ ì”¨ ê´€ë ¨ ì‘ë‹µ
    weather_patterns = {
        "ë‚ ì”¨": [
            "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë–¤ì§€ì— ë”°ë¼ ë‹¤ë¥¸ ìš”ë¦¬ê°€ ìƒê°ë‚˜ì£ ! ğŸ˜Š ë”°ëœ»í•œ ë‚ ì—” ì‹œì›í•œ ìƒëŸ¬ë“œ, ì¶”ìš´ ë‚ ì—” ë”°ëœ»í•œ êµ­ë¬¼ ìš”ë¦¬ëŠ” ì–´ë– ì„¸ìš”?",
            "ë‚ ì”¨ì— ë§ëŠ” ì œì²  ì¬ë£Œë¡œ ìš”ë¦¬í•´ ë³´ì„¸ìš”! ğŸ˜Š ì–´ë–¤ ìš”ë¦¬ë¥¼ ì›í•˜ì‹œëŠ”ì§€ ë§ì”€í•´ ì£¼ì‹œë©´ ì¬ë£Œë¥¼ ì¶”ì²œí•´ ë“œë¦´ê²Œìš”!"
        ]
    }

    # íŒ¨í„´ ë§¤ì¹­ ë° ì‘ë‹µ ì„ íƒ
    all_patterns = {**greeting_patterns, **gratitude_patterns, **wellbeing_patterns, **weather_patterns}

    for pattern, responses in all_patterns.items():
        if pattern in query_lower:
            return random.choice(responses)

    # ê¸°ë³¸ ì‘ë‹µ
    default_responses = [
        "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ì‹ ì„ í•œ ì‹ì¬ë£Œë‚˜ ë§›ìˆëŠ” ë ˆì‹œí”¼ë¥¼ ì°¾ê³  ê³„ì‹ ê°€ìš”?",
        "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì˜¤ëŠ˜ë„ ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”! í•„ìš”í•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!",
        "ë°˜ê°€ì›Œìš”! ğŸ˜Š ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”? ë§›ìˆëŠ” ìš”ë¦¬ë¥¼ ìœ„í•œ ì¬ë£Œë¥¼ ì°¾ì•„ë“œë¦´ê²Œìš”!"
    ]

    return random.choice(default_responses)


def _post_guard_filter(reply: str, history: List[Dict]) -> str:
    """íˆìŠ¤í† ë¦¬ê°€ ì—†ì„ ë•Œ ê³¼ê±° íšŒìƒ/ì¶”ì²œì„ ë§í•˜ëŠ” ë¬¸ì¥ì„ ì°¨ë‹¨"""
    if history:
        return reply
    triggers = ["ì•„ê¹Œ", "ì´ì „ì—", "ë°©ê¸ˆ", "ì¶”ì²œë“œë ¸", "ë§ì”€ë“œë ¸", "ê¸°ì–µí•˜ê³ "]
    if any(t in reply for t in triggers):
        return "ì°½ì„ ë‹«ìœ¼ì…¨ë‹¤ë©´ ëŒ€í™” ê¸°ë¡ì´ ë¹„ì–´ ìˆì–´ìš”. ì–´ë–¤ ë‚´ìš©ì„ ì°¾ê³  ê³„ì…¨ëŠ”ì§€ ì•Œë ¤ì£¼ì‹œë©´ ë°”ë¡œ ì´ì–´ì„œ ë„ì™€ë“œë¦´ê²Œìš”! ğŸ˜Š"
    return reply
