"""
ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°

íˆìŠ¤í† ë¦¬ ë©”ì‹œì§€ í¬ë§·:
{
    "role": "user" | "assistant",
    "content": "ë©”ì‹œì§€ ë‚´ìš©",
    "timestamp": "2025-01-20T10:30:00Z",
    "message_type": "text" | "recipe_request" | "emotional" | "casual",
    "emotion": "happy" | "sad" | "angry" | "stressed" | "neutral",
    "intent": "casual_chat" | "recipe_search" | "product_search"
}

ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ í¬ë§·:
{
    "current_mood": "stressed",
    "recent_topics": ["ì¹œêµ¬ ì‹¸ì›€", "ìŠ¤íŠ¸ë ˆìŠ¤"],
    "preferred_food_types": ["ë§¤ìš´ìŒì‹", "ë”°ëœ»í•œìŒì‹"],
    "conversation_theme": "emotional_support"
}
"""

from datetime import datetime
from typing import Dict, Any, List, Optional
import logging
import json
import os

# ChatState import ì¶”ê°€
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from graph_interfaces import ChatState

logger = logging.getLogger(__name__)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
try:
    import openai
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if openai_api_key:
        openai_client = openai.OpenAI(api_key=openai_api_key)
    else:
        openai_client = None
        logger.warning("OpenAI API key not found. Using fallback analysis.")
except ImportError:
    openai_client = None
    logger.warning("OpenAI package not available.")


def add_to_history(state: ChatState, role: str, content: str, **metadata) -> None:
    """
    ChatStateì˜ conversation_historyì— ë©”ì‹œì§€ ì§ì ‘ ì¶”ê°€ (ë©”ëª¨ë¦¬ ê¸°ë°˜)

    Args:
        state: ChatState ê°ì²´ (ì„¸ì…˜ë³„ë¡œ ì˜ì†ì ìœ¼ë¡œ ê´€ë¦¬ë¨)
        role: 'user' ë˜ëŠ” 'assistant'
        content: ë©”ì‹œì§€ ë‚´ìš©
        **metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (emotion, context ë“±)
    """
    message = {
        "role": role,
        "content": content,
        "timestamp": datetime.now().isoformat(),
        **metadata
    }

    state.conversation_history.append(message)

    logger.info(f"Added to memory history [{len(state.conversation_history)} total]: {role} - {content[:50]}...")

    # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ìë™ ê´€ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
    if len(state.conversation_history) > 20:  # ì•½ê°„ì˜ ë²„í¼ ì œê³µ
        manage_history_length(state, max_messages=15)


def manage_history_length(state: ChatState, max_messages: int = 15) -> None:
    """
    ë©”ëª¨ë¦¬ ê¸°ë°˜ íˆìŠ¤í† ë¦¬ ê¸¸ì´ ê´€ë¦¬

    Args:
        state: ChatState ê°ì²´
        max_messages: ìœ ì§€í•  ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜

    Note:
        ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ íˆìŠ¤í† ë¦¬ë¥¼ ê´€ë¦¬í•˜ì—¬ í† í° ìˆ˜ ì œí•œê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
    """
    if len(state.conversation_history) > max_messages:
        # ìµœê·¼ ë©”ì‹œì§€ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ì²˜ë¦¬)
        trimmed_count = len(state.conversation_history) - max_messages
        state.conversation_history = state.conversation_history[-max_messages:]

        logger.info(f"Memory history trimmed: removed {trimmed_count} old messages, kept {max_messages} recent messages")


def get_recent_context(history: List[Dict], turns: int = 3) -> str:
    """ìµœê·¼ ëŒ€í™” ë§¥ë½ ìš”ì•½ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    if not history:
        return "ìƒˆë¡œìš´ ëŒ€í™”"

    recent_messages = history[-turns*2:] if len(history) >= turns*2 else history
    context_parts = []

    for msg in recent_messages:
        role = "ì‚¬ìš©ì" if msg["role"] == "user" else "ë´‡"
        content = msg["content"][:100] + "..." if len(msg["content"]) > 100 else msg["content"]
        context_parts.append(f"{role}: {content}")

    return "\n".join(context_parts)


def get_contextual_analysis(history: List[Dict], current_query: str) -> Dict[str, Any]:
    """í–¥ìƒëœ ë§¥ë½ ë¶„ì„ - ì´ì „ ì¶”ì²œ ë‚´ì—­ê³¼ ì—°ê´€ì„± íŒŒì•…"""
    if not history:
        return {
            "previous_recommendations": [],
            "conversation_theme": "new_conversation",
            "followup_intent": "none",
            "suggested_alternatives": [],
            "context_summary": "ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘"
        }

    # LLM ê¸°ë°˜ ë§¥ë½ ë¶„ì„ ì‹œë„
    if openai_client:
        return get_contextual_analysis_llm(history, current_query)
    else:
        return get_contextual_analysis_fallback(history, current_query)


# def get_contextual_analysis_llm(history: List[Dict], current_query: str) -> Dict[str, Any]:
#     """LLM ê¸°ë°˜ ê³ ê¸‰ ë§¥ë½ ë¶„ì„ (ì›ë³¸ ë²„ì „ - ë¹ˆ íˆìŠ¤í† ë¦¬ ì²˜ë¦¬ ë¬¸ì œ ìˆìŒ)"""
#     try:
#         recent_context = get_recent_context(history, turns=5)

#         system_prompt = """
# ë‹¹ì‹ ì€ ëŒ€í™” ë§¥ë½ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
# ì´ì „ ëŒ€í™”ì—ì„œ ë´‡ì´ ì¶”ì²œí•œ ë‚´ìš©ê³¼ í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì—°ê´€ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

# ë¶„ì„ í•­ëª©:
# 1. ì´ì „ ì¶”ì²œ ë‚´ì—­ ì¶”ì¶œ
# 2. í˜„ì¬ ì§ˆë¬¸ì˜ ì˜ë„ íŒŒì•… (ìƒˆë¡œìš´ ìš”ì²­ vs ì—°ê´€ ìš”ì²­ vs í›„ì† ì§ˆë¬¸)
# 3. ëŒ€í™” ì£¼ì œ ì—°ì†ì„±
# 4. ì œì•ˆí•  ëŒ€ì•ˆ ì¹´í…Œê³ ë¦¬

# JSON í˜•íƒœë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
# {
#     "previous_recommendations": ["ì´ì „ì— ì¶”ì²œí•œ í•­ëª©ë“¤"],
#     "conversation_theme": "ëŒ€í™” ì£¼ì œ",
#     "followup_intent": "none|similar|alternative|clarification",
#     "suggested_alternatives": ["ì œì•ˆí•  ëŒ€ì•ˆë“¤"],
#     "context_summary": "ë§¥ë½ ìš”ì•½"
# }

# followup_intent ì„¤ëª…:
# - none: ì™„ì „íˆ ìƒˆë¡œìš´ ìš”ì²­
# - similar: ë¹„ìŠ·í•œ ê²ƒ ë” ìš”ì²­
# - alternative: ë‹¤ë¥¸ ëŒ€ì•ˆ ìš”ì²­ ("ë‹¤ë¥¸ XX ì—†ì„ê¹Œ?")
# - clarification: ëª…í™•í™” ìš”ì²­
# """

#         user_prompt = f"""
# ìµœê·¼ ëŒ€í™” ë§¥ë½:
# {recent_context}

# í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸: "{current_query}"

# ìœ„ ì •ë³´ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
# """

#         response = openai_client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": user_prompt}
#             ],
#             max_tokens=300,
#             temperature=0.3
#         )

#         result_text = response.choices[0].message.content.strip()

#         try:
#             result = json.loads(result_text)
#             logger.info(f"LLM contextual analysis: {result}")
#             return result
#         except json.JSONDecodeError:
#             logger.warning(f"Failed to parse LLM contextual analysis: {result_text}")
#             return get_contextual_analysis_fallback(history, current_query)

#     except Exception as e:
#         logger.error(f"LLM contextual analysis failed: {e}")
#         return get_contextual_analysis_fallback(history, current_query)


def get_contextual_analysis_llm(history: List[Dict], current_query: str) -> Dict[str, Any]:
    """LLM ê¸°ë°˜ ê³ ê¸‰ ë§¥ë½ ë¶„ì„ (ë¹ˆ íˆìŠ¤í† ë¦¬ ì²˜ë¦¬ ê°œì„  ë²„ì „)"""
    try:
        recent_context = get_recent_context(history, turns=5)
        is_empty_history = recent_context == "ìƒˆë¡œìš´ ëŒ€í™”"

        system_prompt = """
ğŸš¨ **ì ˆëŒ€ ê·œì¹™ - ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”**:
1. ëŒ€í™” ë§¥ë½ì´ "ìƒˆë¡œìš´ ëŒ€í™”"ì´ë©´ previous_recommendationsëŠ” ë°˜ë“œì‹œ ë¹ˆ ë°°ì—´ []ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
2. í˜„ì¬ ì§ˆë¬¸ë§Œìœ¼ë¡œ ì´ì „ ì¶”ì²œì„ ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
3. ì‹¤ì œ ì´ì „ ëŒ€í™”ì—ì„œ ë´‡ì´ ì¶”ì²œí•œ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
4. ë¹ˆ íˆìŠ¤í† ë¦¬ì—ì„œëŠ” followup_intentê°€ ë°˜ë“œì‹œ "none"ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

ë‹¹ì‹ ì€ ëŒ€í™” ë§¥ë½ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ì „ ëŒ€í™”ì—ì„œ ë´‡ì´ ì¶”ì²œí•œ ë‚´ìš©ê³¼ í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì—°ê´€ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¹ˆ íˆìŠ¤í† ë¦¬ ì²˜ë¦¬ ê·œì¹™:
- ëŒ€í™” ë§¥ë½ì´ "ìƒˆë¡œìš´ ëŒ€í™”"ì¸ ê²½ìš°:
  * previous_recommendations: []
  * followup_intent: "none"
  * conversation_theme: "new_conversation"
  * context_summary: "ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘"

JSON í˜•íƒœë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
{
    "previous_recommendations": ["ì´ì „ì— ì¶”ì²œí•œ í•­ëª©ë“¤ - ë¹ˆ íˆìŠ¤í† ë¦¬ë©´ ë°˜ë“œì‹œ []"],
    "conversation_theme": "ëŒ€í™” ì£¼ì œ",
    "followup_intent": "none|similar|alternative|clarification",
    "suggested_alternatives": ["ì œì•ˆí•  ëŒ€ì•ˆë“¤"],
    "context_summary": "ë§¥ë½ ìš”ì•½"
}

followup_intent ì„¤ëª…:
- none: ì™„ì „íˆ ìƒˆë¡œìš´ ìš”ì²­ (ë¹ˆ íˆìŠ¤í† ë¦¬ëŠ” í•­ìƒ ì´ê²ƒ)
- similar: ë¹„ìŠ·í•œ ê²ƒ ë” ìš”ì²­
- alternative: ë‹¤ë¥¸ ëŒ€ì•ˆ ìš”ì²­ ("ë‹¤ë¥¸ XX ì—†ì„ê¹Œ?")
- clarification: ëª…í™•í™” ìš”ì²­
"""

        user_prompt = f"""
íˆìŠ¤í† ë¦¬ ìƒíƒœ: {"EMPTY (ë¹ˆ íˆìŠ¤í† ë¦¬)" if is_empty_history else "EXISTS (íˆìŠ¤í† ë¦¬ ì¡´ì¬)"}

ìµœê·¼ ëŒ€í™” ë§¥ë½:
{recent_context}

í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸: "{current_query}"

âš ï¸ ì¤‘ìš”: íˆìŠ¤í† ë¦¬ ìƒíƒœê°€ EMPTYë©´ previous_recommendationsëŠ” ë°˜ë“œì‹œ []ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
í˜„ì¬ ì§ˆë¬¸ë§Œìœ¼ë¡œ ì´ì „ ì¶”ì²œì„ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.

ìœ„ ì •ë³´ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=300,
            temperature=0.2  # ë” ì¼ê´€ëœ ì‘ë‹µì„ ìœ„í•´ temperature ë‚®ì¶¤
        )

        result_text = response.choices[0].message.content.strip()

        try:
            result = json.loads(result_text)

            # ë¹ˆ íˆìŠ¤í† ë¦¬ ìƒí™©ì—ì„œ ì‘ë‹µ ê²€ì¦ ë° ê°•ì œ ìˆ˜ì •
            if is_empty_history:
                if result.get("previous_recommendations") and len(result["previous_recommendations"]) > 0:
                    logger.warning(f"ë¹ˆ íˆìŠ¤í† ë¦¬ì¸ë° previous_recommendationsê°€ ì¡´ì¬í•¨: {result['previous_recommendations']} -> [] ê°•ì œ ìˆ˜ì •")
                    result["previous_recommendations"] = []

                if result.get("followup_intent") != "none":
                    logger.warning(f"ë¹ˆ íˆìŠ¤í† ë¦¬ì¸ë° followup_intentê°€ 'none'ì´ ì•„ë‹˜: {result['followup_intent']} -> 'none' ê°•ì œ ìˆ˜ì •")
                    result["followup_intent"] = "none"

                result["conversation_theme"] = "new_conversation"
                result["context_summary"] = "ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘"

            logger.info(f"LLM contextual analysis (ê°œì„  ë²„ì „): {result}")
            return result

        except json.JSONDecodeError:
            logger.warning(f"Failed to parse LLM contextual analysis: {result_text}")
            return get_contextual_analysis_fallback(history, current_query)

    except Exception as e:
        logger.error(f"LLM contextual analysis failed: {e}")
        return get_contextual_analysis_fallback(history, current_query)


def get_contextual_analysis_fallback(history: List[Dict], current_query: str) -> Dict[str, Any]:
    """í´ë°± ë§¥ë½ ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
    # ìµœê·¼ ë´‡ ë©”ì‹œì§€ì—ì„œ ì¶”ì²œ ë‚´ì—­ ì¶”ì¶œ
    previous_recommendations = []
    conversation_theme = "general"

    recent_bot_messages = [
        msg["content"] for msg in history[-10:]
        if msg["role"] == "assistant"
    ]

    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì²œ ë‚´ì—­ ì¶”ì¶œ
    for content in recent_bot_messages:
        content_lower = content.lower()
        if any(keyword in content_lower for keyword in ["ìŠ¤í”„", "ìˆ˜í”„", "soup"]):
            if "ì¹˜í‚¨" in content_lower:
                previous_recommendations.append("ì¹˜í‚¨ìˆ˜í”„")
            if "í† ë§ˆí† " in content_lower:
                previous_recommendations.append("í† ë§ˆí† ìˆ˜í”„")
            if "ë°”ì§ˆ" in content_lower:
                previous_recommendations.append("ë°”ì§ˆìˆ˜í”„")

        # ê¸°íƒ€ ìŒì‹ ì¹´í…Œê³ ë¦¬ë“¤
        food_keywords = {
            "ë¼ë©´": ["ë¼ë©´", "ë©´", "noodle"],
            "ê¹€ì¹˜ì°Œê°œ": ["ê¹€ì¹˜ì°Œê°œ", "ì°Œê°œ"],
            "ì¹˜í‚¨": ["ì¹˜í‚¨", "chicken", "ë‹­"],
            "í”¼ì": ["í”¼ì", "pizza"]
        }

        for food, keywords in food_keywords.items():
            if any(k in content_lower for k in keywords):
                previous_recommendations.append(food)

    # í›„ì† ì˜ë„ íŒŒì•…
    query_lower = current_query.lower()
    followup_intent = "none"

    if any(pattern in query_lower for pattern in ["ë‹¤ë¥¸", "ë˜", "ë§ê³ ", "else", "other"]):
        followup_intent = "alternative"
    elif any(pattern in query_lower for pattern in ["ë¹„ìŠ·í•œ", "similar", "ê°™ì€"]):
        followup_intent = "similar"
    elif any(pattern in query_lower for pattern in ["ë­", "ì–´ë–¤", "what", "?"]):
        followup_intent = "clarification"

    # ëŒ€ì•ˆ ì œì•ˆ
    suggested_alternatives = []
    if "ìŠ¤í”„" in query_lower or "ìˆ˜í”„" in query_lower:
        suggested_alternatives = ["ì˜¥ìˆ˜ìˆ˜ìˆ˜í”„", "í¬ë¦¼ìˆ˜í”„", "ë²„ì„¯ìˆ˜í”„", "í˜¸ë°•ìˆ˜í”„"]
    elif "ë¼ë©´" in query_lower:
        suggested_alternatives = ["ìš°ë™", "ì«„ë©´", "ëƒ‰ë©´", "ë¹„ë¹”ë©´"]
    elif "ì¹˜í‚¨" in query_lower:
        suggested_alternatives = ["ì‚¼ê²¹ì‚´", "ê°ˆë¹„", "ë¶ˆê³ ê¸°", "ìŠ¤í…Œì´í¬"]

    return {
        "previous_recommendations": list(set(previous_recommendations)),  # ì¤‘ë³µ ì œê±°
        "conversation_theme": conversation_theme,
        "followup_intent": followup_intent,
        "suggested_alternatives": suggested_alternatives,
        "context_summary": f"ì´ì „ ì¶”ì²œ: {', '.join(previous_recommendations[:3]) if previous_recommendations else 'ì—†ìŒ'}"
    }


def analyze_user_emotion(query: str, history: List[Dict]) -> Dict[str, Any]:
    """LLM ê¸°ë°˜ ì‚¬ìš©ì ê°ì • ìƒíƒœì™€ ìƒí™© ë¶„ì„"""

    if openai_client:
        return analyze_user_emotion_llm(query, history)
    else:
        return analyze_user_emotion_fallback(query, history)


def analyze_user_emotion_llm(query: str, history: List[Dict]) -> Dict[str, Any]:
    """LLMì„ ì‚¬ìš©í•œ ë™ì  ê°ì • ë¶„ì„"""
    try:
        # ìµœê·¼ ëŒ€í™” ë§¥ë½ êµ¬ì„±
        recent_context = get_recent_context(history, turns=3) if history else "ìƒˆë¡œìš´ ëŒ€í™”"

        system_prompt = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê°ì •ê³¼ ìƒí™©ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ë©”ì‹œì§€ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë³´ê³  ê°ì • ìƒíƒœë¥¼ ì •í™•íˆ íŒŒì•…í•´ì£¼ì„¸ìš”.

ê°ì • ë¶„ë¥˜:
- happy: ê¸°ì¨, í–‰ë³µ, ì¦ê±°ì›€, ë§Œì¡±
- sad: ìŠ¬í””, ìš°ìš¸, ì¢Œì ˆ, ì‹¤ë§
- angry: í™”ë‚¨, ì§œì¦, ë¶„ë…¸, ì–µìš¸í•¨
- stressed: ìŠ¤íŠ¸ë ˆìŠ¤, ì••ë°•ê°, ë°”ì¨, ë¶€ë‹´
- lonely: ì™¸ë¡œì›€, ê³ ë…, ì“¸ì“¸í•¨
- tired: í”¼ê³¤, ì§€ì¹¨, ë¬´ê¸°ë ¥
- excited: í¥ë¶„, ê¸°ëŒ€, ì„¤ë ˜
- worried: ê±±ì •, ë¶ˆì•ˆ, ì—¼ë ¤
- neutral: ì¤‘ë¦½ì , ì¼ìƒì  ëŒ€í™”

ìƒí™© ë¶„ì„ë„ í¬í•¨í•´ì„œ JSON í˜•íƒœë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
{
    "primary_emotion": "ê°ì •ëª…",
    "confidence": 0.0-1.0,
    "context": "êµ¬ì²´ì ì¸ ìƒí™© ì„¤ëª…",
    "intensity": "low|medium|high",
    "food_mood": "ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìŒì‹ì´ ë„ì›€ë ì§€"
}
"""

        user_prompt = f"""
ìµœê·¼ ëŒ€í™” ë§¥ë½:
{recent_context}

í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€: "{query}"

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ê°ì •ê³¼ ìƒí™©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=200,
            temperature=0.3
        )

        result_text = response.choices[0].message.content.strip()

        # JSON íŒŒì‹± ì‹œë„
        try:
            result = json.loads(result_text)
            logger.info(f"LLM emotion analysis: {result}")
            return result
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse LLM response as JSON: {result_text}")
            return analyze_user_emotion_fallback(query, history)

    except Exception as e:
        logger.error(f"LLM emotion analysis failed: {e}")
        return analyze_user_emotion_fallback(query, history)


def analyze_user_emotion_fallback(query: str, history: List[Dict]) -> Dict[str, Any]:
    """LLM ì‹¤íŒ¨ ì‹œ í´ë°± í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„"""
    emotion_keywords = {
        "sad": ["ìŠ¬í¼", "ìš°ìš¸í•´", "í˜ë“¤ì–´", "ì‹¸ì› ì–´", "ì†ìƒí•´", "ëˆˆë¬¼", "ì•„íŒŒ"],
        "stressed": ["ìŠ¤íŠ¸ë ˆìŠ¤", "í”¼ê³¤í•´", "ì§€ì³", "ë°”ë¹ ", "í˜ë“¤ì–´", "ë‹µë‹µí•´"],
        "happy": ["ì¢‹ì•„", "ê¸°ë»", "í–‰ë³µí•´", "ì¦ê±°ì›Œ", "ì‹ ë‚˜", "ì™„ì „", "ìµœê³ "],
        "angry": ["í™”ë‚˜", "ì§œì¦", "ë¶„í•´", "ì–µìš¸í•´", "ì—´ë°›", "ë¹¡ì³"],
        "lonely": ["ì™¸ë¡œì›Œ", "í˜¼ì", "ì“¸ì“¸í•´", "ì‹¬ì‹¬í•´"],
        "tired": ["í”¼ê³¤", "ì¡¸ë ¤", "ì ", "ì§€ì³"]
    }

    query_lower = query.lower()
    detected_emotions = []

    for emotion, keywords in emotion_keywords.items():
        for keyword in keywords:
            if keyword in query_lower:
                detected_emotions.append(emotion)
                break

    primary_emotion = detected_emotions[0] if detected_emotions else "neutral"
    confidence = 0.8 if detected_emotions else 0.3
    context = extract_context_from_history(history, query)

    return {
        "primary_emotion": primary_emotion,
        "confidence": confidence,
        "context": context,
        "intensity": "medium",
        "food_mood": "ì¼ë°˜ì ì¸ ìŒì‹"
    }


def extract_context_from_history(history: List[Dict], current_query: str) -> str:
    """íˆìŠ¤í† ë¦¬ì—ì„œ ìƒí™© ë§¥ë½ ì¶”ì¶œ"""
    if not history:
        return "ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘"

    recent_user_messages = [
        msg["content"] for msg in history[-6:]
        if msg["role"] == "user"
    ]

    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë§¥ë½ ì¶”ì¶œ
    context_keywords = {
        "ì¹œêµ¬": "ì¹œêµ¬ ê´€ê³„",
        "ì‹¸ì›€": "ê°ˆë“± ìƒí™©",
        "ì§ì¥": "ì§ì¥ ìŠ¤íŠ¸ë ˆìŠ¤",
        "ê³µë¶€": "í•™ì—… ìŠ¤íŠ¸ë ˆìŠ¤",
        "ê°€ì¡±": "ê°€ì¡± ë¬¸ì œ",
        "ì—°ì• ": "ì—°ì•  ê³ ë¯¼"
    }

    all_text = " ".join(recent_user_messages + [current_query])

    for keyword, context in context_keywords.items():
        if keyword in all_text:
            return context

    return "ì¼ìƒ ëŒ€í™”"


def get_empathy_response(emotion: str, context: str, intensity: str = "medium") -> str:
    """LLM ê¸°ë°˜ ê³µê° ë©”ì‹œì§€ ìƒì„±"""

    if openai_client:
        return get_empathy_response_llm(emotion, context, intensity)
    else:
        return get_empathy_response_fallback(emotion, context)


def get_empathy_response_llm(emotion: str, context: str, intensity: str) -> str:
    """LLMì„ ì‚¬ìš©í•œ ë™ì  ê³µê° ë©”ì‹œì§€ ìƒì„±"""
    try:
        system_prompt = """
ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ê³µê°ì ì¸ ì‹ ì„ ì‹í’ˆ ì‡¼í•‘ëª°ì˜ ì±—ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê°ì •ê³¼ ìƒí™©ì— ë§ëŠ” ì§„ì‹¬ì–´ë¦° ê³µê° ë©”ì‹œì§€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ê°€ì´ë“œë¼ì¸:
- ì§„ì‹¬ì–´ë¦° ê³µê°ê³¼ ìœ„ë¡œ í‘œí˜„
- ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš© (ê³¼í•˜ì§€ ì•Šê²Œ)
- 1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
- ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ í†¤
- ìƒí™©ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´

ê°ì • ê°•ë„ì— ë”°ë¥¸ í†¤ ì¡°ì ˆ:
- low: ê°€ë²¼ìš´ ê³µê°
- medium: ì ë‹¹í•œ ê³µê°ê³¼ ìœ„ë¡œ
- high: ê¹Šì€ ê³µê°ê³¼ ì§„ì‹¬ì–´ë¦° ìœ„ë¡œ
"""

        user_prompt = f"""
ì‚¬ìš©ì ê°ì •: {emotion}
ìƒí™© ë§¥ë½: {context}
ê°ì • ê°•ë„: {intensity}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³µê° ë©”ì‹œì§€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ë©”ì‹œì§€ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=100,
            temperature=0.7
        )

        result = response.choices[0].message.content.strip()
        logger.info(f"LLM empathy response: {result}")
        return result

    except Exception as e:
        logger.error(f"LLM empathy response failed: {e}")
        return get_empathy_response_fallback(emotion, context)


def get_empathy_response_fallback(emotion: str, context: str) -> str:
    """í´ë°± ê³µê° ë©”ì‹œì§€ ìƒì„±"""
    empathy_templates = {
        "sad": [
            "ì•„ì´ê³ , ë§ˆìŒì´ ë§ì´ ì•„í”„ì‹œê² ì–´ìš”. ğŸ˜¢",
            "í˜ë“  ì¼ì´ ìˆìœ¼ì…¨êµ°ìš”. ì†ìƒí•˜ì…¨ì„ í…ë°...",
            "ë§ˆìŒì´ ë¬´ê±°ìš°ì‹œê² ë„¤ìš”. ğŸ˜”"
        ],
        "stressed": [
            "ì•„ì´ê³ , ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ìœ¼ì…¨êµ°ìš”. ğŸ˜Ÿ ì •ë§ í˜ë“œì‹œê² ì–´ìš”.",
            "ë°”ì˜ê³  í˜ë“  í•˜ë£¨ì˜€ë‚˜ ë³´ë„¤ìš”.",
            "ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ë§ì´ ìŒ“ì´ì…¨ë‚˜ ë´ìš”. ğŸ˜”"
        ],
        "angry": [
            "í™”ê°€ ë§ì´ ë‚˜ì…¨êµ°ìš”. ğŸ˜¤ ì¶©ë¶„íˆ ì´í•´í•´ìš”.",
            "ì •ë§ ì–µìš¸í•˜ê³  í™”ë‚˜ì…¨ì„ ê²ƒ ê°™ì•„ìš”.",
            "ê·¸ëŸ° ì¼ì´ ìˆìœ¼ë©´ ë‹¹ì—°íˆ í™”ê°€ ë‚˜ì£ ."
        ],
        "lonely": [
            "ì™¸ë¡œìš°ì…¨êµ°ìš”. ğŸ˜¢ í˜¼ì ìˆëŠ” ì‹œê°„ì´ ê¸¸ë©´ ê·¸ëŸ´ ìˆ˜ ìˆì–´ìš”.",
            "ì“¸ì“¸í•œ ê¸°ë¶„ì´ì‹œêµ°ìš”. ì´í•´í•´ìš”."
        ],
        "tired": [
            "ë§ì´ í”¼ê³¤í•˜ì‹œê² ì–´ìš”. ğŸ˜´ ëª¸ì´ í˜ë“œì‹œì£ ?",
            "ì§€ì¹˜ì…¨êµ°ìš”. í‘¹ ì‰¬ì…”ì•¼ê² ì–´ìš”."
        ]
    }

    import random
    messages = empathy_templates.get(emotion, empathy_templates.get("stressed", ["í˜ë“œì…¨êµ°ìš”."]))
    return random.choice(messages)


def recommend_food_by_emotion(emotion: str, context: str, food_mood: str = "") -> Dict[str, Any]:
    """LLM ê¸°ë°˜ ê°ì •ê³¼ ìƒí™©ì— ë§ëŠ” ìŒì‹ ì¶”ì²œ"""

    if openai_client:
        return recommend_food_by_emotion_llm(emotion, context, food_mood)
    else:
        return recommend_food_by_emotion_fallback(emotion, context)


def recommend_food_by_emotion_llm(emotion: str, context: str, food_mood: str) -> Dict[str, Any]:
    """LLMì„ ì‚¬ìš©í•œ ë™ì  ìŒì‹ ì¶”ì²œ"""
    try:
        system_prompt = """
ë‹¹ì‹ ì€ ì‹ ì„ ì‹í’ˆ ì‡¼í•‘ëª°ì˜ ìŒì‹ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê°ì •ê³¼ ìƒí™©ì— ë§ëŠ” ìŒì‹ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ì‹¬ë¦¬í•™ì  ê·¼ê±°ì™€ ì˜ì–‘í•™ì  ê´€ì ì„ ê³ ë ¤í•´ì„œ:
- ê°ì • ìƒíƒœì— ë„ì›€ì´ ë˜ëŠ” ìŒì‹ ì¢…ë¥˜
- êµ¬ì²´ì ì¸ ìš”ë¦¬ëª… 3-5ê°œ
- ì¶”ì²œ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…

JSON í˜•íƒœë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
{
    "types": ["ìŒì‹ ì¹´í…Œê³ ë¦¬1", "ì¹´í…Œê³ ë¦¬2"],
    "keywords": ["êµ¬ì²´ì  ìš”ë¦¬ëª…1", "ìš”ë¦¬ëª…2", "ìš”ë¦¬ëª…3"],
    "reason": "ì¶”ì²œ ì´ìœ  ì„¤ëª…"
}
"""

        user_prompt = f"""
ì‚¬ìš©ì ê°ì •: {emotion}
ìƒí™© ë§¥ë½: {context}
ìŒì‹ ê¸°ë¶„: {food_mood}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ìŒì‹ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=200,
            temperature=0.5
        )

        result_text = response.choices[0].message.content.strip()

        try:
            result = json.loads(result_text)
            logger.info(f"LLM food recommendation: {result}")
            return result
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse LLM food response as JSON: {result_text}")
            return recommend_food_by_emotion_fallback(emotion, context)

    except Exception as e:
        logger.error(f"LLM food recommendation failed: {e}")
        return recommend_food_by_emotion_fallback(emotion, context)


def recommend_food_by_emotion_fallback(emotion: str, context: str) -> Dict[str, Any]:
    """í´ë°± ìŒì‹ ì¶”ì²œ"""
    food_recommendations = {
        "sad": {
            "types": ["ë”°ëœ»í•œìŒì‹", "ë‹¬ì½¤í•œìŒì‹", "ë¶€ë“œëŸ¬ìš´ìŒì‹"],
            "keywords": ["ë”°ëœ»í•œ ì£½", "ì‚¼ê³„íƒ•", "í•«ì´ˆì½œë¦¿", "ì¼€ì´í¬", "ì•„ì´ìŠ¤í¬ë¦¼"],
            "reason": "ë”°ëœ»í•˜ê³  ë‹¬ì½¤í•œ ìŒì‹ì´ ë§ˆìŒì„ ìœ„ë¡œí•´ì¤„ ê±°ì˜ˆìš”"
        },
        "stressed": {
            "types": ["ë§¤ìš´ìŒì‹", "ë”°ëœ»í•œêµ­ë¬¼", "ì‹œì›í•œìŒì‹"],
            "keywords": ["ë§¤ìš´ ë–¡ë³¶ì´", "ê¹€ì¹˜ì°Œê°œ", "ë¼ë©´", "ë§¤ìš´ê°ˆë¹„ì°œ", "ëƒ‰ë©´"],
            "reason": "ë§¤ìš´ ìŒì‹ì´ ì—”ëŒí•€ ë¶„ë¹„ë¥¼ ë„ì™€ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œì— ë„ì›€ì´ ë¼ìš”"
        },
        "angry": {
            "types": ["ë§¤ìš´ìŒì‹", "ë°”ì‚­í•œìŒì‹"],
            "keywords": ["ë§¤ìš´ì¹˜í‚¨", "ë–¡ë³¶ì´", "ë§ˆë¼íƒ•", "ë°”ì‚­í•œ íŠ€ê¹€"],
            "reason": "ë§¤ìš´ ìŒì‹ìœ¼ë¡œ í™”ë¥¼ ë‹¬ë˜ë³´ì„¸ìš”! ë°”ì‚­í•œ ì‹ê°ë„ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œì— ì¢‹ì•„ìš”"
        },
        "lonely": {
            "types": ["ë”°ëœ»í•œìŒì‹", "ê°„í¸ì‹"],
            "keywords": ["ë¼ë©´", "ê¹€ì¹˜ì°Œê°œ", "ì°œë‹­", "í”¼ì"],
            "reason": "í˜¼ìì„œë„ ë§›ìˆê²Œ ë¨¹ì„ ìˆ˜ ìˆëŠ” ë”°ëœ»í•œ ìŒì‹ ì–´ë– ì„¸ìš”?"
        },
        "tired": {
            "types": ["ê°„í¸ì‹", "ì˜ì–‘ì‹", "ì—ë„ˆì§€ì‹"],
            "keywords": ["ì‚¼ê³„íƒ•", "ì „ë³µì£½", "ë³´ì–‘ì‹", "ê°„í¸ë„ì‹œë½"],
            "reason": "í”¼ê³¤í•  ë•ŒëŠ” ì˜ì–‘ ê°€ë“í•œ ë³´ì–‘ì‹ì´ ìµœê³ ì˜ˆìš”"
        },
        "happy": {
            "types": ["ì¶•í•˜ìŒì‹", "ë‹¬ì½¤í•œìŒì‹"],
            "keywords": ["ì¼€ì´í¬", "í”¼ì", "ì¹˜í‚¨", "ì´ˆë°¥"],
            "reason": "ì¢‹ì€ ì¼ì´ ìˆìœ¼ì‹œêµ°ìš”! ì¶•í•˜ ê²¸ ë§›ìˆëŠ” ìŒì‹ ì–´ë– ì„¸ìš”?"
        }
    }

    return food_recommendations.get(emotion, food_recommendations["stressed"])


def update_user_context(state, emotion_analysis: Dict[str, Any]) -> None:
    """ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì—…ë°ì´íŠ¸"""
    if not hasattr(state, 'user_context'):
        state.user_context = {}

    # í˜„ì¬ ê°ì • ìƒíƒœ ì—…ë°ì´íŠ¸
    state.user_context["current_mood"] = emotion_analysis["primary_emotion"]

    # ìµœê·¼ ì£¼ì œ ì—…ë°ì´íŠ¸
    if "recent_topics" not in state.user_context:
        state.user_context["recent_topics"] = []

    context = emotion_analysis["context"]
    if context and context not in state.user_context["recent_topics"]:
        state.user_context["recent_topics"].append(context)
        # ìµœê·¼ 5ê°œë§Œ ìœ ì§€
        if len(state.user_context["recent_topics"]) > 5:
            state.user_context["recent_topics"] = state.user_context["recent_topics"][-5:]

    # ëŒ€í™” í…Œë§ˆ ì„¤ì •
    if emotion_analysis["primary_emotion"] in ["sad", "stressed", "angry"]:
        state.user_context["conversation_theme"] = "emotional_support"
    elif emotion_analysis["primary_emotion"] == "happy":
        state.user_context["conversation_theme"] = "celebration"
    else:
        state.user_context["conversation_theme"] = "casual"

    logger.info(f"Updated user context: {state.user_context}")


# íŒŒì¼ I/O ê¸°ë°˜ í•¨ìˆ˜ë“¤ ì œê±°ë¨ (2025-01-16)
# ì´ì œ conversation_historyëŠ” ChatStateì—ì„œ ë©”ëª¨ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ìë™ ê´€ë¦¬ë¨
# utils/session_manager.pyì˜ get_or_create_session_state()ë¥¼ í†µí•´ ì˜ì†ì„± í™•ë³´

# ===== ë ˆì‹œí”¼ ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ (ChatState.user_context ê¸°ë°˜) =====

def save_recipe_search_result(state: ChatState, search_context: Dict[str, Any]) -> None:
    """
    ë ˆì‹œí”¼ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ChatState.user_contextì— ì €ì¥ (ë©”ëª¨ë¦¬ ê¸°ë°˜ íœ˜ë°œì„±)

    Args:
        state: ChatState ê°ì²´
        search_context: ê²€ìƒ‰ ê²°ê³¼ ë° ë©”íƒ€ë°ì´í„°
            - query_type: "specific_dish" | "general_menu" (LLMì´ ë¶„ë¥˜)
            - original_query: ì‚¬ìš©ì ì›ë¬¸ ì¿¼ë¦¬
            - search_query: ì‹¤ì œ ê²€ìƒ‰ì— ì‚¬ìš©ëœ ì¿¼ë¦¬
            - results: [{"title": "...", "url": "..."}] í˜•íƒœì˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            - search_type: "initial" | "alternative"
            - dish_category: LLMì´ ë¶„ì„í•œ ìŒì‹ ì¹´í…Œê³ ë¦¬ (ì„ íƒì‚¬í•­)
            - cuisine_type: LLMì´ ë¶„ì„í•œ ìš”ë¦¬ ìœ í˜• (ì„ íƒì‚¬í•­)
    """
    try:
        # user_context ì´ˆê¸°í™”
        if "recipe_search_history" not in state.user_context:
            state.user_context["recipe_search_history"] = []

        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        search_context["timestamp"] = datetime.now().isoformat()

        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        state.user_context["recipe_search_history"].append(search_context)

        # ìµœëŒ€ 5ê°œë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        if len(state.user_context["recipe_search_history"]) > 5:
            state.user_context["recipe_search_history"] = state.user_context["recipe_search_history"][-5:]

        logger.info(f"ë ˆì‹œí”¼ ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ ì €ì¥: {search_context.get('search_query', 'unknown')} " +
                   f"(ì´ {len(state.user_context['recipe_search_history'])}ê°œ)")

    except Exception as e:
        logger.error(f"ë ˆì‹œí”¼ ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")


def get_recent_recipe_search_context(state: ChatState, current_query: str) -> Dict[str, Any]:
    """
    í˜„ì¬ ì¿¼ë¦¬ì™€ ì—°ê´€ëœ ìµœê·¼ ë ˆì‹œí”¼ ê²€ìƒ‰ ë§¥ë½ ë°˜í™˜ (LLM ê¸°ë°˜ ì—°ê´€ì„± ë¶„ì„)

    Args:
        state: ChatState ê°ì²´
        current_query: í˜„ì¬ ì‚¬ìš©ì ì¿¼ë¦¬

    Returns:
        Dict with:
        - has_previous_search: bool
        - most_recent_search: Dict (ê°€ì¥ ìµœê·¼ ê²€ìƒ‰ ì •ë³´)
        - related_searches: List[Dict] (ì—°ê´€ëœ ê²€ìƒ‰ë“¤)
        - context_summary: str (ë§¥ë½ ìš”ì•½)
    """
    try:
        history = state.user_context.get("recipe_search_history", [])

        if not history:
            return {
                "has_previous_search": False,
                "most_recent_search": None,
                "related_searches": [],
                "context_summary": "ì´ì „ ë ˆì‹œí”¼ ê²€ìƒ‰ ê¸°ë¡ ì—†ìŒ"
            }

        most_recent = history[-1]  # ê°€ì¥ ìµœê·¼ ê²€ìƒ‰

        # LLMì„ ì‚¬ìš©í•œ ì—°ê´€ì„± ë¶„ì„ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if openai_client:
            related_searches = _analyze_search_relatedness_llm(history, current_query)
        else:
            related_searches = _analyze_search_relatedness_fallback(history, current_query)

        context_summary = _generate_context_summary(most_recent, related_searches, current_query)

        return {
            "has_previous_search": True,
            "most_recent_search": most_recent,
            "related_searches": related_searches,
            "context_summary": context_summary
        }

    except Exception as e:
        logger.error(f"ë ˆì‹œí”¼ ê²€ìƒ‰ ë§¥ë½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "has_previous_search": False,
            "most_recent_search": None,
            "related_searches": [],
            "context_summary": "ë§¥ë½ ë¶„ì„ ì‹¤íŒ¨"
        }


def analyze_search_intent_with_history(state: ChatState, current_query: str) -> Dict[str, Any]:
    """
    LLMì´ íˆìŠ¤í† ë¦¬ë¥¼ ì°¸ì¡°í•˜ì—¬ ì¬ê²€ìƒ‰ ì˜ë„ ë¶„ì„

    Args:
        state: ChatState ê°ì²´
        current_query: í˜„ì¬ ì‚¬ìš©ì ì¿¼ë¦¬

    Returns:
        Dict with:
        - is_alternative_search: bool (ì¬ê²€ìƒ‰ ì—¬ë¶€)
        - intent_scope: "same_dish" | "different_menu" | "clarification"
        - previous_dish: str (ì´ì „ ê²€ìƒ‰ ìŒì‹ëª…)
        - similarity_level: float (0.0-1.0, ì´ì „ ê²€ìƒ‰ê³¼ ìœ ì‚¬ë„)
        - search_strategy: str (ì¶”ì²œ ê²€ìƒ‰ ì „ëµ)
    """
    try:
        search_context = get_recent_recipe_search_context(state, current_query)

        if not search_context["has_previous_search"]:
            return {
                "is_alternative_search": False,
                "intent_scope": "new_search",
                "previous_dish": None,
                "similarity_level": 0.0,
                "search_strategy": "INITIAL_SEARCH"
            }

        if openai_client:
            return _analyze_search_intent_llm(search_context, current_query)
        else:
            return _analyze_search_intent_fallback(search_context, current_query)

    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì˜ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            "is_alternative_search": False,
            "intent_scope": "error",
            "previous_dish": None,
            "similarity_level": 0.0,
            "search_strategy": "FALLBACK_SEARCH"
        }


def generate_alternative_search_strategy(previous_search: Dict[str, Any], current_query: str) -> Dict[str, Any]:
    """
    LLMì´ ì´ì „ ê²€ìƒ‰ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì•ˆ ê²€ìƒ‰ ì „ëµ ìƒì„±

    Args:
        previous_search: ì´ì „ ê²€ìƒ‰ ì •ë³´
        current_query: í˜„ì¬ ì¿¼ë¦¬

    Returns:
        Dict with:
        - strategy_type: "SAME_DISH_ALTERNATIVE" | "CATEGORY_EXPANSION" | "DIFFERENT_MENU"
        - alternative_queries: List[str] (ëŒ€ì•ˆ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤)
        - exclude_urls: List[str] (ì œì™¸í•  URLë“¤)
        - search_modifiers: List[str] (ê²€ìƒ‰ ìˆ˜ì •ìë“¤, ì˜ˆ: "ê°„ë‹¨í•œ", "ë§¤ìš´")
        - reasoning: str (ì „ëµ ì„ íƒ ì´ìœ )
    """
    try:
        if openai_client:
            return _generate_alternative_strategy_llm(previous_search, current_query)
        else:
            return _generate_alternative_strategy_fallback(previous_search, current_query)

    except Exception as e:
        logger.error(f"ëŒ€ì•ˆ ê²€ìƒ‰ ì „ëµ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "strategy_type": "FALLBACK_SEARCH",
            "alternative_queries": [previous_search.get("search_query", "ë ˆì‹œí”¼")],
            "exclude_urls": [],
            "search_modifiers": [],
            "reasoning": f"ì „ëµ ìƒì„± ì˜¤ë¥˜ë¡œ ì¸í•œ í´ë°±: {e}"
        }


# ===== LLM ê¸°ë°˜ ë¶„ì„ í•¨ìˆ˜ë“¤ =====

def _analyze_search_relatedness_llm(history: List[Dict], current_query: str) -> List[Dict]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ì—°ê´€ì„± ë¶„ì„"""
    try:
        # ìµœê·¼ 3ê°œ ê²€ìƒ‰ë§Œ ë¶„ì„ (í† í° íš¨ìœ¨ì„±)
        recent_history = history[-3:]

        system_prompt = """
ë‹¹ì‹ ì€ ë ˆì‹œí”¼ ê²€ìƒ‰ íŒ¨í„´ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ì™€ í˜„ì¬ ì§ˆë¬¸ì„ ë³´ê³ , ì—°ê´€ëœ ì´ì „ ê²€ìƒ‰ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”.

ì—°ê´€ì„± ê¸°ì¤€:
1. ë™ì¼í•œ ìŒì‹ì˜ ë‹¤ë¥¸ ë ˆì‹œí”¼ (ì˜ˆ: "ê¹€ì¹˜ì°Œê°œ" ê´€ë ¨)
2. ê°™ì€ ì¹´í…Œê³ ë¦¬ ìŒì‹ (ì˜ˆ: "í•œì‹", "ì°Œê°œë¥˜", "ë§¤ìš´ìŒì‹")
3. ë¹„ìŠ·í•œ ì¡°ë¦¬ë²•ì´ë‚˜ ì¬ë£Œ

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "related_searches": [
        {
            "search_query": "ê´€ë ¨ ê²€ìƒ‰ì–´",
            "relatedness_score": 0.0-1.0,
            "relation_type": "same_dish|same_category|similar_ingredient|different"
        }
    ]
}
"""

        history_summary = "\n".join([
            f"- {search.get('search_query', 'unknown')}: {search.get('query_type', 'unknown')}"
            for search in recent_history
        ])

        user_prompt = f"""
ê²€ìƒ‰ íˆìŠ¤í† ë¦¬:
{history_summary}

í˜„ì¬ ì§ˆë¬¸: "{current_query}"

ìœ„ ì •ë³´ë¥¼ ë¶„ì„í•´ì„œ ì—°ê´€ëœ ê²€ìƒ‰ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=300,
            temperature=0.3
        )

        result_text = response.choices[0].message.content.strip()
        result = json.loads(result_text)

        logger.info(f"LLM ê²€ìƒ‰ ì—°ê´€ì„± ë¶„ì„: {len(result.get('related_searches', []))}ê°œ")
        return result.get("related_searches", [])

    except Exception as e:
        logger.error(f"LLM ê²€ìƒ‰ ì—°ê´€ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
        return _analyze_search_relatedness_fallback(history, current_query)


def _analyze_search_relatedness_fallback(history: List[Dict], current_query: str) -> List[Dict]:
    """í´ë°± ê²€ìƒ‰ ì—°ê´€ì„± ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
    related = []
    current_lower = current_query.lower()

    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ì—°ê´€ì„± íŒë‹¨
    for search in history[-3:]:  # ìµœê·¼ 3ê°œë§Œ
        search_query = search.get("search_query", "").lower()

        # ì™„ì „ ì¼ì¹˜ (ë™ì¼ ìŒì‹)
        if search_query in current_lower or current_lower in search_query:
            related.append({
                "search_query": search.get("search_query", ""),
                "relatedness_score": 0.9,
                "relation_type": "same_dish"
            })
        # ë¶€ë¶„ ì¼ì¹˜ (ê´€ë ¨ ì¹´í…Œê³ ë¦¬)
        elif any(word in current_lower for word in search_query.split()):
            related.append({
                "search_query": search.get("search_query", ""),
                "relatedness_score": 0.6,
                "relation_type": "same_category"
            })

    return related


def _analyze_search_intent_llm(search_context: Dict, current_query: str) -> Dict[str, Any]:
    """LLM ê¸°ë°˜ ê²€ìƒ‰ ì˜ë„ ë¶„ì„"""
    try:
        recent_search = search_context["most_recent_search"]

        system_prompt = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì˜ë„ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ì „ ë ˆì‹œí”¼ ê²€ìƒ‰ê³¼ í˜„ì¬ ì§ˆë¬¸ì„ ë³´ê³ , ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì˜ë„ ë¶„ë¥˜:
- same_dish: ê°™ì€ ìŒì‹ì˜ ë‹¤ë¥¸ ë ˆì‹œí”¼ ("ë‹¤ë¥¸ ê¹€ì¹˜ì°Œê°œ ë ˆì‹œí”¼")
- different_menu: ì™„ì „íˆ ë‹¤ë¥¸ ë©”ë‰´ ("ë‹¤ë¥¸ ìš”ë¦¬ ì¶”ì²œ")
- clarification: ëª…í™•í™” ìš”ì²­ ("ë” ìì„¸íˆ", "ì¬ë£Œê°€ ë­ì•¼")

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{
    "is_alternative_search": true/false,
    "intent_scope": "same_dish|different_menu|clarification",
    "previous_dish": "ì´ì „ ê²€ìƒ‰í•œ ìŒì‹ëª…",
    "similarity_level": 0.0-1.0,
    "search_strategy": "SAME_DISH_ALTERNATIVE|CATEGORY_EXPANSION|DIFFERENT_MENU|CLARIFICATION",
    "confidence": 0.0-1.0
}
"""

        user_prompt = f"""
ì´ì „ ê²€ìƒ‰:
- ì¿¼ë¦¬: "{recent_search.get('original_query', '')}"
- ê²€ìƒ‰ì–´: "{recent_search.get('search_query', '')}"
- ìŒì‹ ì¢…ë¥˜: {recent_search.get('query_type', 'unknown')}

í˜„ì¬ ì§ˆë¬¸: "{current_query}"

ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=200,
            temperature=0.2
        )

        result_text = response.choices[0].message.content.strip()
        result = json.loads(result_text)

        logger.info(f"LLM ê²€ìƒ‰ ì˜ë„ ë¶„ì„: {result.get('intent_scope', 'unknown')}")
        return result

    except Exception as e:
        logger.error(f"LLM ê²€ìƒ‰ ì˜ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return _analyze_search_intent_fallback(search_context, current_query)


def _analyze_search_intent_fallback(search_context: Dict, current_query: str) -> Dict[str, Any]:
    """í´ë°± ê²€ìƒ‰ ì˜ë„ ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
    recent_search = search_context["most_recent_search"]
    query_lower = current_query.lower()

    # ì¬ê²€ìƒ‰ í‚¤ì›Œë“œ ê°ì§€
    alternative_keywords = ["ë‹¤ë¥¸", "ë˜", "ë³„ì˜", "ìƒˆë¡œìš´", "ë‹¤ì‹œ", "ë§ê³ ", "else", "other", "another"]
    is_alternative = any(keyword in query_lower for keyword in alternative_keywords)

    if not is_alternative:
        return {
            "is_alternative_search": False,
            "intent_scope": "new_search",
            "previous_dish": None,
            "similarity_level": 0.0,
            "search_strategy": "INITIAL_SEARCH"
        }

    # ë™ì¼ ìŒì‹ vs ë‹¤ë¥¸ ë©”ë‰´ íŒë‹¨
    previous_dish = recent_search.get("search_query", "")
    dish_in_query = previous_dish.lower() in query_lower if previous_dish else False

    if dish_in_query or recent_search.get("query_type") == "specific_dish":
        intent_scope = "same_dish"
        strategy = "SAME_DISH_ALTERNATIVE"
        similarity = 0.8
    else:
        intent_scope = "different_menu"
        strategy = "DIFFERENT_MENU"
        similarity = 0.2

    return {
        "is_alternative_search": True,
        "intent_scope": intent_scope,
        "previous_dish": previous_dish,
        "similarity_level": similarity,
        "search_strategy": strategy
    }


def _generate_alternative_strategy_llm(previous_search: Dict, current_query: str) -> Dict[str, Any]:
    """LLM ê¸°ë°˜ ëŒ€ì•ˆ ê²€ìƒ‰ ì „ëµ ìƒì„±"""
    try:
        system_prompt = """
ë‹¹ì‹ ì€ ë ˆì‹œí”¼ ê²€ìƒ‰ ì „ëµì„ ì„¤ê³„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ì „ ê²€ìƒ‰ ê²°ê³¼ì™€ í˜„ì¬ ì‚¬ìš©ì ìš”ì²­ì„ ë³´ê³ , ìµœì ì˜ ëŒ€ì•ˆ ê²€ìƒ‰ ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

ì „ëµ ì¢…ë¥˜:
1. SAME_DISH_ALTERNATIVE: ê°™ì€ ìŒì‹ì˜ ë‹¤ë¥¸ ë ˆì‹œí”¼
   - ê¸°ì¡´ ê²°ê³¼ ì œì™¸í•˜ê³  ê²€ìƒ‰ ìˆ˜ì •ì ì¶”ê°€
2. CATEGORY_EXPANSION: ì¹´í…Œê³ ë¦¬ í™•ì¥
   - ê´€ë ¨ ìŒì‹ ì¹´í…Œê³ ë¦¬ë¡œ í™•ì¥
3. DIFFERENT_MENU: ì™„ì „ ë‹¤ë¥¸ ë©”ë‰´
   - ë‹¤ë¥¸ ìŒì‹ ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{
    "strategy_type": "ì „ëµ íƒ€ì…",
    "alternative_queries": ["ëŒ€ì•ˆ ì¿¼ë¦¬1", "ì¿¼ë¦¬2"],
    "exclude_urls": ["ì œì™¸í•  URLë“¤"],
    "search_modifiers": ["ìˆ˜ì •ì1", "ìˆ˜ì •ì2"],
    "reasoning": "ì „ëµ ì„ íƒ ì´ìœ "
}
"""

        exclude_urls = [result.get("url", "") for result in previous_search.get("results", [])]

        user_prompt = f"""
ì´ì „ ê²€ìƒ‰:
- ê²€ìƒ‰ì–´: "{previous_search.get('search_query', '')}"
- ìŒì‹ ì¢…ë¥˜: {previous_search.get('query_type', 'unknown')}
- ê²°ê³¼ ê°œìˆ˜: {len(previous_search.get('results', []))}ê°œ

í˜„ì¬ ìš”ì²­: "{current_query}"

ì œì™¸í•  URLë“¤: {len(exclude_urls)}ê°œ

ìµœì ì˜ ëŒ€ì•ˆ ê²€ìƒ‰ ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”.
"""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=300,
            temperature=0.4
        )

        result_text = response.choices[0].message.content.strip()
        result = json.loads(result_text)

        # exclude_urls ì¶”ê°€
        result["exclude_urls"] = exclude_urls

        logger.info(f"LLM ëŒ€ì•ˆ ì „ëµ ìƒì„±: {result.get('strategy_type', 'unknown')}")
        return result

    except Exception as e:
        logger.error(f"LLM ëŒ€ì•ˆ ì „ëµ ìƒì„± ì‹¤íŒ¨: {e}")
        return _generate_alternative_strategy_fallback(previous_search, current_query)


def _generate_alternative_strategy_fallback(previous_search: Dict, current_query: str) -> Dict[str, Any]:
    """í´ë°± ëŒ€ì•ˆ ì „ëµ ìƒì„±"""
    previous_query = previous_search.get("search_query", "")
    exclude_urls = [result.get("url", "") for result in previous_search.get("results", [])]

    # ê°„ë‹¨í•œ ìˆ˜ì •ì ì¶”ê°€
    modifiers = ["ê°„ë‹¨í•œ", "ì‰¬ìš´", "ë§¤ì½¤í•œ", "ì „í†µ", "ì§‘ì—ì„œ ë§Œë“œëŠ”", "íŠ¹ë³„í•œ"]
    alternative_queries = [f"{modifier} {previous_query}" for modifier in modifiers[:3]]

    return {
        "strategy_type": "SAME_DISH_ALTERNATIVE",
        "alternative_queries": alternative_queries,
        "exclude_urls": exclude_urls,
        "search_modifiers": modifiers[:3],
        "reasoning": "í´ë°± ì „ëµ: ê¸°ë³¸ ìˆ˜ì •ìë¥¼ ì¶”ê°€í•œ ë™ì¼ ìŒì‹ ê²€ìƒ‰"
    }


def _generate_context_summary(most_recent: Dict, related_searches: List, current_query: str) -> str:
    """ë§¥ë½ ìš”ì•½ ìƒì„±"""
    recent_dish = most_recent.get("search_query", "unknown")
    recent_time = most_recent.get("timestamp", "")

    if related_searches:
        return f"ìµœê·¼ '{recent_dish}' ê²€ìƒ‰ í›„ ì—°ê´€ ê²€ìƒ‰ {len(related_searches)}ê°œ ë°œê²¬"
    else:
        return f"ìµœê·¼ '{recent_dish}' ê²€ìƒ‰, í˜„ì¬ ì§ˆë¬¸ê³¼ ì—°ê´€ì„± ë‚®ìŒ"


# hjs ìˆ˜ì •: ë„ë©”ì¸ ì „ë°˜ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ íˆìŠ¤í† ë¦¬ í—¬í¼ ì¶”ê°€ # ë©€í‹°í„´ ê¸°ëŠ¥
def get_recent_messages_by_intents(state: ChatState, intents: List[str], limit: int = 5) -> List[Dict[str, Any]]:
    """ì§€ì •ëœ intent ëª©ë¡ì— í•´ë‹¹í•˜ëŠ” ìµœê·¼ ë©”ì‹œì§€ ì¶”ì¶œ"""
    history = getattr(state, "conversation_history", [])
    if not history:
        return []

    collected: List[Dict[str, Any]] = []
    for message in reversed(history):
        if message.get("intent") in intents:
            collected.append(message)
            if len(collected) >= limit:
                break

    return list(reversed(collected))


def summarize_product_search_with_history(state: ChatState, current_query: str, limit: int = 3) -> Dict[str, Any]:
    """ìƒí’ˆ ê²€ìƒ‰ ë©€í‹°í„´ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½"""
    recent_messages = get_recent_messages_by_intents(state, ["product_search", "product_recommendation"], limit)
    recent_queries = [msg.get("content", "") for msg in recent_messages]

    last_slots = {}
    recent_candidates: List[Dict[str, Any]] = []  # hjs ìˆ˜ì • # ë©€í‹°í„´ ê¸°ëŠ¥
    for msg in reversed(recent_messages):
        slots = msg.get("slots") or {}
        if slots and not last_slots:
            last_slots = slots
        search_payload = msg.get("search") or {}  # hjs ìˆ˜ì • # ë©€í‹°í„´ ê¸°ëŠ¥
        if search_payload.get("candidates") and not recent_candidates:
            recent_candidates = search_payload.get("candidates")

    return {
        "has_previous_search": bool(recent_messages),
        "recent_queries": recent_queries,
        "last_slots": last_slots,
        "recent_candidates": recent_candidates,
        "current_query": current_query
    }


def summarize_cart_actions_with_history(state: ChatState, limit: int = 5) -> Dict[str, Any]:
    """ì¥ë°”êµ¬ë‹ˆ ê´€ë ¨ ë©€í‹°í„´ ë§¥ë½ ìš”ì•½"""
    intents = ["cart_add", "cart_remove", "cart_view", "checkout"]
    recent_actions = get_recent_messages_by_intents(state, intents, limit)

    normalized_actions = []
    last_cart_snapshot: Dict[str, Any] = {}  # hjs ìˆ˜ì • # ë©€í‹°í„´ ê¸°ëŠ¥
    for action in recent_actions:
        normalized_actions.append({
            "intent": action.get("intent"),
            "content": action.get("content"),
            "timestamp": action.get("timestamp"),
            "slots": action.get("slots", {})
        })

        if not last_cart_snapshot and action.get("cart"):
            last_cart_snapshot = action.get("cart")
        if action.get("meta") and action["meta"].get("added_items"):
            last_cart_snapshot.setdefault("last_added_items", action["meta"].get("added_items"))  # hjs ìˆ˜ì • # ë©€í‹°í„´ ê¸°ëŠ¥

    last_action = normalized_actions[-1] if normalized_actions else None

    return {
        "has_cart_activity": bool(normalized_actions),
        "recent_actions": normalized_actions,
        "last_action": last_action,
        "selected_products": last_action.get("slots", {}).get("items") if last_action else [],
        "last_cart_snapshot": last_cart_snapshot
    }


def summarize_cs_history(state: ChatState, limit: int = 5) -> Dict[str, Any]:
    """ê³ ê°ì„¼í„°(ë°°ì†¡/í™˜ë¶ˆ ë“±) ëŒ€í™” ë§¥ë½ ìš”ì•½"""
    intents = ["cs_inquiry", "cs_followup", "refund", "delivery"]
    recent_cs = get_recent_messages_by_intents(state, intents, limit)

    topics = []
    for msg in recent_cs:
        topic = msg.get("cs_topic") or msg.get("intent")
        if topic:
            topics.append(topic)

    return {
        "has_cs_history": bool(recent_cs),
        "recent_topics": topics,
        "recent_messages": recent_cs
    }


def build_global_context_snapshot(state: ChatState, current_query: str) -> Dict[str, Any]:
    """ê° ë„ë©”ì¸ì˜ ë§¥ë½ ìš”ì•½ì„ í†µí•©í•œ ìŠ¤ëƒ…ìƒ· ìƒì„±"""
    return {
        "product": summarize_product_search_with_history(state, current_query),
        "cart": summarize_cart_actions_with_history(state),
        "cs": summarize_cs_history(state)
    }
